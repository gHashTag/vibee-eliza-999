# üß† –ü–ê–ú–Ø–¢–¨ –ê–ì–ï–ù–¢–û–í

## üìç –í–´ –ó–î–ï–°–¨

```
[‚úì]‚îÅ‚îÅ‚îÅ[‚úì]‚îÅ‚îÅ‚îÅ[‚úì]‚îÅ‚îÅ‚îÅ[‚úì]‚îÅ‚îÅ‚îÅ[‚úì]‚îÅ‚îÅ‚îÅ[‚óè]‚îÅ‚îÅ‚îÅ[ ]‚îÅ‚îÅ‚îÅ[‚úì]
 01    02    03    04    05    07    08    06
                         –ü–ê–ú–Ø–¢–¨         –ë—É–¥—É—â–µ–µ
```

**–ü—Ä–æ–≥—Ä–µ—Å—Å:** 75% (6 –∏–∑ 8 –≥–ª–∞–≤) | **–í—Ä–µ–º—è:** ~2 —á–∞—Å–∞ —á—Ç–µ–Ω–∏—è

---

## üéØ –¶–ï–õ–¨ –ì–õ–ê–í–´

–ü–æ—Å–ª–µ –ø—Ä–æ—á—Ç–µ–Ω–∏—è –≤—ã –±—É–¥–µ—Ç–µ –ø–æ–Ω–∏–º–∞—Ç—å:
- üß† **–ü–æ—á–µ–º—É –ø–∞–º—è—Ç—å –∫—Ä–∏—Ç–∏—á–Ω–∞** –¥–ª—è AI-–∞–≥–µ–Ω—Ç–æ–≤
- üìù **Short-Term Memory** (–∫–æ–Ω—Ç–µ–∫—Å—Ç 4K-1M tokens)
- üíæ **Long-Term Memory** (Vector DB: Pinecone, Chroma)
- üé≠ **–¢–∏–ø—ã –ø–∞–º—è—Ç–∏** (Episodic, Semantic, Procedural)
- üîß **Context Engineering** (LangChain 2025)
- üèóÔ∏è **–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã** –ø–∞–º—è—Ç–∏ (–∫–∞–∫ —ç—Ç–æ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ)

---

## üö® –ì–õ–ê–í–ù–ê–Ø –ü–†–û–ë–õ–ï–ú–ê

> **"–ü–æ—á–µ–º—É ChatGPT –∑–∞–±—ã–≤–∞–µ—Ç —Ç–æ, —á—Ç–æ —è –≥–æ–≤–æ—Ä–∏–ª 10 —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–∞–∑–∞–¥?"**

**–ü—Ä–∏–º–µ—Ä:**
```
–°–æ–æ–±—â–µ–Ω–∏–µ 1:
User: "–ú–µ–Ω—è –∑–æ–≤—É—Ç –ê–ª–µ–∫—Å–µ–π, —è –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –Ω–∞ Python"
ChatGPT: "–ü—Ä–∏—è—Ç–Ω–æ –ø–æ–∑–Ω–∞–∫–æ–º–∏—Ç—å—Å—è, –ê–ª–µ–∫—Å–µ–π! –ß–µ–º –º–æ–≥—É –ø–æ–º–æ—á—å?"

... [20 —Å–æ–æ–±—â–µ–Ω–∏–π –æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–∏] ...

–°–æ–æ–±—â–µ–Ω–∏–µ 22:
User: "–ö–∞–∫ –º–µ–Ω—è –∑–æ–≤—É—Ç?"
ChatGPT: "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –ø–æ–º–Ω—é –≤–∞—à–µ –∏–º—è –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏–π."

‚Üí –ó–ê–ë–´–õ!
```

**–ü–æ—á–µ–º—É?** ‚Üí –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å (Short-Term Memory)

---

## üìñ –¢–ò–ü–´ –ü–ê–ú–Ø–¢–ò (–ü—Å–∏—Ö–æ–ª–æ–≥–∏—è ‚Üí AI)

**–í –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ (—á–µ–ª–æ–≤–µ–∫):**
1. **–°–µ–Ω—Å–æ—Ä–Ω–∞—è –ø–∞–º—è—Ç—å** ‚Äî –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥—ã (–≤—Å–ø—ã—à–∫–∞ —Å–≤–µ—Ç–∞)
2. **Short-Term Memory (STM)** ‚Äî —Å–µ–∫—É–Ω–¥—ã/–º–∏–Ω—É—Ç—ã (—Ç–µ–ª–µ—Ñ–æ–Ω–Ω—ã–π –Ω–æ–º–µ—Ä)
3. **Long-Term Memory (LTM)** ‚Äî –≥–æ–¥—ã/–≤—Å—è –∂–∏–∑–Ω—å (–¥–µ—Ç—Å–∫–∏–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è)

**–í AI-–∞–≥–µ–Ω—Ç–∞—Ö:**
1. **–ö–æ–Ω—Ç–µ–∫—Å—Ç (Context Window)** = STM (4K-1M tokens)
2. **Vector Database** = LTM (–±–µ—Å–∫–æ–Ω–µ—á–Ω–æ, –ø–æ–∫–∞ –µ—Å—Ç—å –º–µ—Å—Ç–æ)
3. **Parameters (–≤–µ—Å–∞ –º–æ–¥–µ–ª–∏)** = "–í—Ä–æ–∂–¥–µ–Ω–Ω—ã–µ" –∑–Ω–∞–Ω–∏—è

---

## 1Ô∏è‚É£ SHORT-TERM MEMORY (–ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ)

### üìñ –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï

**Short-Term Memory (STM)** ‚Äî —ç—Ç–æ –ø–∞–º—è—Ç—å –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞** (context window) –º–æ–¥–µ–ª–∏.

**–§–æ—Ä–º—É–ª–∞:**
```
Context Window = –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ tokens, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–¥–µ–ª—å "–≤–∏–¥–∏—Ç" –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
```

**–†–∞–∑–º–µ—Ä—ã (2024):**
```
GPT-3.5: 4K tokens (~3,000 —Å–ª–æ–≤)
GPT-4: 8K/32K/128K tokens
Claude 3: 200K tokens (~150,000 —Å–ª–æ–≤)
Gemini 1.5: 1M tokens (~750,000 —Å–ª–æ–≤)
```

---

### üß† –ö–ê–ö –†–ê–ë–û–¢–ê–ï–¢

**–ü—Ä–∏–º–µ—Ä (GPT-4, 8K tokens):**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Context Window (8K tokens)  ‚îÇ
‚îÇ                              ‚îÇ
‚îÇ  System Prompt:   500 tokens ‚îÇ
‚îÇ  User Message 1:  200 tokens ‚îÇ
‚îÇ  AI Response 1:   300 tokens ‚îÇ
‚îÇ  User Message 2:  150 tokens ‚îÇ
‚îÇ  AI Response 2:   250 tokens ‚îÇ
‚îÇ  ...                         ‚îÇ
‚îÇ  User Message N:  100 tokens ‚îÇ
‚îÇ  [–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞...]       ‚îÇ
‚îÇ                              ‚îÇ
‚îÇ  –ò–¢–û–ì–û: 7,500 / 8,000 tokens ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–°–ª–µ–¥—É—é—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ:
User: "..." (200 tokens)
‚Üí 7,500 + 200 = 7,700 tokens OK

–ï—â–µ –æ–¥–Ω–æ:
User: "..." (500 tokens)
‚Üí 7,700 + 500 = 8,200 tokens ‚ùå –ü–ï–†–ï–ü–û–õ–ù–ï–ù–ò–ï!

–†–µ—à–µ–Ω–∏–µ:
‚Üí –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è (User Message 1 + AI Response 1)
‚Üí –û—Å–≤–æ–±–æ–¥–∏—Ç—å 500 tokens
```

---

### üí° –ü–†–û–ë–õ–ï–ú–´

**–ü—Ä–æ–±–ª–µ–º–∞ 1: –ó–∞–±—ã–≤–∞–Ω–∏–µ**
```
–î–∏–∞–ª–æ–≥ –Ω–∞ 100 —Å–æ–æ–±—â–µ–Ω–∏–π
–ö–æ–Ω—Ç–µ–∫—Å—Ç: 8K tokens
‚Üí –ú–æ–¥–µ–ª—å "–∑–∞–±—ã–≤–∞–µ—Ç" –ø–µ—Ä–≤—ã–µ 70 —Å–æ–æ–±—â–µ–Ω–∏–π

User: "–ü–æ–º–Ω–∏—à—å, —á—Ç–æ —è –≥–æ–≤–æ—Ä–∏–ª –≤ –Ω–∞—á–∞–ª–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞?"
ChatGPT: "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –ø–æ–º–Ω—é"
```

**–ü—Ä–æ–±–ª–µ–º–∞ 2: –°—Ç–æ–∏–º–æ—Å—Ç—å**
```
Gemini 1.5: 1M tokens –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
–°—Ç–æ–∏–º–æ—Å—Ç—å: $0.001 / 1K tokens
‚Üí $1 –∑–∞ –û–î–ù–û —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –ø–æ–ª–Ω—ã–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º

100 —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –¥–µ–Ω—å = $100/–¥–µ–Ω—å = $3,000/–º–µ—Å—è—Ü
‚Üí –î–æ—Ä–æ–≥–æ!
```

---

### ‚úÖ –†–ï–®–ï–ù–ò–Ø

#### 1. –°–∂–∞—Ç–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (Summarization)

**–ò–¥–µ—è:** –°—Ç–∞—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è ‚Üí –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ


**–ü—Ä–∏–º–µ—Ä:**
```
–ë—ã–ª–æ (20 —Å–æ–æ–±—â–µ–Ω–∏–π, 7K tokens):
User: "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ Python"
AI: [300 tokens –æ–± –∏—Å—Ç–æ—Ä–∏–∏ Python]
User: "–ö–∞–∫–∏–µ –µ—Å—Ç—å —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏?"
AI: [400 tokens –ø—Ä–æ Django, Flask...]
...

–°–∂–∞—Ç–æ (—Ä–µ–∑—é–º–µ, 500 tokens):
"–û–±—Å—É–∂–¥–∞–ª–∏: Python (–∏—Å—Ç–æ—Ä–∏—è, —Å–∏–Ω—Ç–∞–∫—Å–∏—Å), —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∏ (Django, Flask)"

–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: 6,500 tokens ‚Üí –¢–µ–ø–µ—Ä—å –º–æ–∂–µ–º –ø—Ä–æ–¥–æ–ª–∂–∞—Ç—å –¥–∏–∞–ª–æ–≥!
```

---

#### 2. Sliding Window

**–ò–¥–µ—è:** –•—Ä–∞–Ω–∏—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π


**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
- ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –¢–µ—Ä—è–µ—Ç —Å—Ç–∞—Ä—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –ü–û–õ–ù–û–°–¢–¨–Æ

---

## 2Ô∏è‚É£ LONG-TERM MEMORY (–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å)

### üìñ –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï

**Long-Term Memory (LTM)** ‚Äî —ç—Ç–æ –ø–∞–º—è—Ç—å **–≤–Ω–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞**, —Ö—Ä–∞–Ω—è—â–∞—è—Å—è –≤ **–≤–Ω–µ—à–Ω–µ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö** (–æ–±—ã—á–Ω–æ Vector DB).

**–§–æ—Ä–º—É–ª–∞:**
```
Long-Term Memory = Vector Database (Embeddings)
```

**–ö–ª—é—á–µ–≤–∞—è –∏–¥–µ—è:**
- –°—Ç–∞—Ä—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è ‚Üí Embeddings (–≤–µ–∫—Ç–æ—Ä—ã)
- –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ Vector DB (Pinecone, Chroma, Weaviate)
- –ü—Ä–∏ –Ω–æ–≤–æ–º –∑–∞–ø—Ä–æ—Å–µ ‚Üí –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ (semantic search)
- –î–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç

---

### üß† –ö–ê–ö –†–ê–ë–û–¢–ê–ï–¢

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ User Query          ‚îÇ
‚îÇ "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ RAG"  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Embedding Model     ‚îÇ
‚îÇ text ‚Üí vector       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Vector Database     ‚îÇ
‚îÇ Search similar      ‚îÇ
‚îÇ vectors             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Top-K Results       ‚îÇ
‚îÇ (3-5 relevant docs) ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM (GPT-4)         ‚îÇ
‚îÇ Context: Query +    ‚îÇ
‚îÇ Retrieved docs      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
           ‚îÇ
           ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Answer              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### üí° –ü–†–ò–ú–ï–†: CHATGPT MEMORY

**ChatGPT (—Å Memory feature, 2024):**

```
–°–µ—Å—Å–∏—è 1 (01.01.2024):
User: "–Ø –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –Ω–∞ Python, —Ä–∞–±–æ—Ç–∞—é –≤ —Å—Ç–∞—Ä—Ç–∞–ø–µ"
ChatGPT: "–ü–æ–Ω—è–ª, —Å–æ—Ö—Ä–∞–Ω—é"
‚Üí [–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ LTM: "User = Python programmer, startup"]

–°–µ—Å—Å–∏—è 2 (15.02.2024, –Ω–æ–≤—ã–π –¥–∏–∞–ª–æ–≥):
User: "–ü–æ—Å–æ–≤–µ—Ç—É–π –∫–Ω–∏–≥—É –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é"
ChatGPT: "–¢–∞–∫ –∫–∞–∫ –≤—ã –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –Ω–∞ Python..."
‚Üí [–ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ LTM: –ø—Ä–æ—Ñ–µ—Å—Å–∏—è]

–°–µ—Å—Å–∏—è 3 (20.03.2024):
User: "–ö–∞–∫ –º–Ω–µ —É–ª—É—á—à–∏—Ç—å –Ω–∞–≤—ã–∫–∏?"
ChatGPT: "–î–ª—è Python-—Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞ –≤ —Å—Ç–∞—Ä—Ç–∞–ø–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É—é..."
‚Üí [–ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ LTM: –ø—Ä–æ—Ñ–µ—Å—Å–∏—è + —Å—Ç–∞—Ä—Ç–∞–ø]
```

**–ö–∞–∫ —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç:**

---

### üõ†Ô∏è VECTOR DATABASES

**–ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã:**

#### 1. Pinecone (–æ–±–ª–∞—á–Ω—ã–π)


**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ Managed service (–Ω–µ –Ω—É–∂–Ω–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞—Ç—å)
- ‚úÖ –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –ü–ª–∞—Ç–Ω—ã–π ($70+/–º–µ—Å—è—Ü)

---

#### 2. Chroma (open-source)


**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π (open-source)
- ‚úÖ –ü—Ä–æ—Å—Ç–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞

**–ù–µ–¥–æ—Å—Ç–∞—Ç–∫–∏:**
- ‚ùå –ù—É–∂–Ω–æ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞—Ç—å —Å–∞–º–æ–º—É

---

#### 3. Weaviate (–≥–∏–±—Ä–∏–¥–Ω—ã–π)


---

## 3Ô∏è‚É£ –¢–ò–ü–´ –ü–ê–ú–Ø–¢–ò (Episodic, Semantic, Procedural)

### 1Ô∏è‚É£ Episodic Memory (–≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è)

**–ß—Ç–æ:** –ü–∞–º—è—Ç—å –æ **–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã—Ö —Å–æ–±—ã—Ç–∏—è—Ö** (–∫—Ç–æ, —á—Ç–æ, –∫–æ–≥–¥–∞, –≥–¥–µ).

**–ü—Ä–∏–º–µ—Ä—ã:**
```
"–í—á–µ—Ä–∞ —è –≤—Å—Ç—Ä–µ—Ç–∏–ª—Å—è —Å –∫–ª–∏–µ–Ω—Ç–æ–º –î–∂–æ–Ω–æ–º –∏ –æ–±—Å—É–¥–∏–ª –∫–æ–Ω—Ç—Ä–∞–∫—Ç"
"–í –ø—Ä–æ—à–ª–æ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —É–ø–æ–º—è–Ω—É–ª, —á—Ç–æ –ª—é–±–∏—Ç Python"
"15.03.2024 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø–æ–ø—Ä–æ—Å–∏–ª –ø–æ–º–æ—â—å —Å Django"
```

**–•—Ä–∞–Ω–µ–Ω–∏–µ:**

---

### 2Ô∏è‚É£ Semantic Memory (–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è)

**–ß—Ç–æ:** –ü–∞–º—è—Ç—å –æ **—Ñ–∞–∫—Ç–∞—Ö –∏ –∑–Ω–∞–Ω–∏—è—Ö** (—á—Ç–æ —Ç–∞–∫–æ–µ X, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Y).

**–ü—Ä–∏–º–µ—Ä—ã:**
```
"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Å—Ç –Ω–∞ Python"
"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ AI-—Å—Ç–∞—Ä—Ç–∞–ø–µ"
"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç Flask –≤–º–µ—Å—Ç–æ Django"
```

**–•—Ä–∞–Ω–µ–Ω–∏–µ:**

---

### 3Ô∏è‚É£ Procedural Memory (–ü—Ä–æ—Ü–µ–¥—É—Ä–Ω–∞—è)

**–ß—Ç–æ:** –ü–∞–º—è—Ç—å –æ **–∫–∞–∫ –¥–µ–ª–∞—Ç—å** (–∞–ª–≥–æ—Ä–∏—Ç–º—ã, –Ω–∞–≤—ã–∫–∏).

**–ü—Ä–∏–º–µ—Ä—ã:**
```
"–î–ª—è —ç—Ç–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å—Ç–∏–ª—å '–∫—Ä–∞—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã'"
"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ø—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ—Ç –ø—Ä–∏–º–µ—Ä—ã –∫–æ–¥–∞ –≤–º–µ—Å—Ç–æ —Ç–µ–æ—Ä–∏–∏"
"–í—Å–µ–≥–¥–∞ —Å–ø—Ä–∞—à–∏–≤–∞—Ç—å —É—Ç–æ—á–Ω—è—é—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –ø–µ—Ä–µ–¥ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"
```

**–•—Ä–∞–Ω–µ–Ω–∏–µ:**

---

## 4Ô∏è‚É£ CONTEXT ENGINEERING (2025)

### üìñ –û–ü–†–ï–î–ï–õ–ï–ù–ò–ï

**Context Engineering** ‚Äî —ç—Ç–æ –∏—Å–∫—É—Å—Å—Ç–≤–æ **—É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º** –¥–ª—è –¥–ª–∏–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ (100+ —Å–æ–æ–±—â–µ–Ω–∏–π).

**–ò—Å—Ç–æ—á–Ω–∏–∫:** *"Long-Horizon Language Modeling"* (LangChain blog, 2025)

---

### üß† –ê–†–•–ò–¢–ï–ö–¢–£–†–ê

**Hybrid Memory System:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      MEMORY MANAGER                 ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Short-Term Memory (STM)      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Last 10 messages           ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Total: 3K tokens           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Summary                      ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Messages 1-50: "Discussed  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ   Python, Django..."         ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Total: 500 tokens          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Long-Term Memory (LTM)       ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Vector DB (all history)    ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Retrieval: Top-3 relevant  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ - Total: 1K tokens           ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ                                     ‚îÇ
‚îÇ  Total Context: 4.5K / 8K tokens   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### üí° –†–ï–ê–õ–ò–ó–ê–¶–ò–Ø (LangChain)


---

## 5Ô∏è‚É£ –†–ï–ê–õ–¨–ù–´–ï –ü–†–ò–ú–ï–†–´

### –ü—Ä–∏–º–µ—Ä 1: Character.AI

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å:** –ü–æ–º–Ω–∏—Ç **–≤—Å—é –∏—Å—Ç–æ—Ä–∏—é** –æ–±—â–µ–Ω–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º (–º–µ—Å—è—Ü—ã).

**–ö–∞–∫:**
```
–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞:
- Short-Term: –ü–æ—Å–ª–µ–¥–Ω–∏–µ 20 —Å–æ–æ–±—â–µ–Ω–∏–π (–≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ)
- Long-Term: Vector DB (Pinecone) —Å –í–°–ï –∏—Å—Ç–æ—Ä–∏–µ–π
- Retrieval: –ü—Ä–∏ –∫–∞–∂–¥–æ–º —Å–æ–æ–±—â–µ–Ω–∏–∏ –∏—â–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–æ—à–ª—ã–µ –¥–∏–∞–ª–æ–≥–∏

–ü—Ä–∏–º–µ—Ä:
User (—Å–µ–≥–æ–¥–Ω—è): "–ü–æ–º–Ω–∏—à—å, –º—ã –æ–±—Å—É–∂–¥–∞–ª–∏ –∫–Ω–∏–≥–∏ 2 –º–µ—Å—è—Ü–∞ –Ω–∞–∑–∞–¥?"
Character.AI: "–î–∞! –¢—ã —Å–ø—Ä–∞—à–∏–≤–∞–ª –ø—Ä–æ sci-fi, —è —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–ª '–î—é–Ω—É'"
‚Üí [–ò–∑–≤–ª–µ—á–µ–Ω–æ –∏–∑ Vector DB: –¥–∏–∞–ª–æ–≥ –æ—Ç 2 –º–µ—Å—è—Ü–µ–≤ –Ω–∞–∑–∞–¥]
```

---

### –ü—Ä–∏–º–µ—Ä 2: Notion AI

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å:** –ü–æ–º–Ω–∏—Ç **—Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤—Å–µ—Ö –≤–∞—à–∏—Ö –∑–∞–º–µ—Ç–æ–∫** –≤ Notion.

**–ö–∞–∫:**
```
1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è: –í—Å–µ –∑–∞–º–µ—Ç–∫–∏ ‚Üí Embeddings ‚Üí Vector DB
2. –ü—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ: "Summarize my notes about AI" ‚Üí –ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∑–∞–º–µ—Ç–æ–∫
3. –ö–æ–Ω—Ç–µ–∫—Å—Ç: –ù–∞–π–¥–µ–Ω–Ω—ã–µ –∑–∞–º–µ—Ç–∫–∏ + –∑–∞–ø—Ä–æ—Å ‚Üí LLM
4. –û—Ç–≤–µ—Ç: –†–µ–∑—é–º–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –í–ê–®–ò–• –∑–∞–º–µ—Ç–æ–∫ (–Ω–µ –æ–±—â–∏—Ö –∑–Ω–∞–Ω–∏–π)
```

---

### –ü—Ä–∏–º–µ—Ä 3: Replit Ghostwriter

**–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å:** –ü–æ–º–Ω–∏—Ç **–≤–µ—Å—å –≤–∞—à –∫–æ–¥** –≤ –ø—Ä–æ–µ–∫—Ç–µ.

**–ö–∞–∫:**
```
1. Codebase ‚Üí Embeddings (code2vec)
2. –ü—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è:
   - –¢–µ–∫—É—â–∏–π —Ñ–∞–π–ª (–∫–æ–Ω—Ç–µ–∫—Å—Ç)
   - –ü–æ—Ö–æ–∂–∏–µ —Ñ–∞–π–ª—ã (–∏–∑ Vector DB)
   ‚Üí LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫–æ–¥

–†–µ–∑—É–ª—å—Ç–∞—Ç: –ê–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ —É—á–∏—Ç—ã–≤–∞–µ—Ç –í–ê–® —Å—Ç–∏–ª—å –∫–æ–¥–∞
```

---

## ‚ö†Ô∏è –ü–†–û–ë–õ–ï–ú–´ –ò –í–´–ó–û–í–´

### 1Ô∏è‚É£ Privacy (–ö–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å)

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
–í—Å–µ –≤–∞—à–∏ —Å–æ–æ–±—â–µ–Ω–∏—è ‚Üí Vector DB ‚Üí –•—Ä–∞–Ω–∏—Ç—å—Å—è –≤–µ—á–Ω–æ
–ß—Ç–æ –µ—Å–ª–∏ —É—Ç–µ—á–∫–∞?
```

**–†–µ—à–µ–Ω–∏—è:**
- **On-Device Vector DB** (—Ö—Ä–∞–Ω–∏—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ)
- **Encryption** (—à–∏—Ñ—Ä–æ–≤–∞–Ω–∏–µ)
- **Federated Learning** (–¥–∞–Ω–Ω—ã–µ –Ω–µ –ø–æ–∫–∏–¥–∞—é—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ)

---

### 2Ô∏è‚É£ Data Leakage (–£—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö –º–µ–∂–¥—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º–∏)

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
User A: "–ú–æ–π –ø–∞—Ä–æ–ª—å: qwerty123"
‚Üí –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ Vector DB

User B: "–ö–∞–∫–∏–µ –ø–∞—Ä–æ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç –ª—é–¥–∏?"
‚Üí Vector DB –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç "qwerty123" –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ User A

‚ùå –£–¢–ï–ß–ö–ê!
```

**–†–µ—à–µ–Ω–∏–µ:**
- **User Isolation** ‚Äî —Ñ–∏–ª—å—Ç—Ä –ø–æ user_id
- **Sensitive Data Detection** ‚Äî –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ø–∞—Ä–æ–ª–∏, —Å–µ–∫—Ä–µ—Ç—ã

---

### 3Ô∏è‚É£ Cost (–°—Ç–æ–∏–º–æ—Å—Ç—å)

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
Vector DB (Pinecone):
- Storage: $0.096/GB/–º–µ—Å—è—Ü
- Queries: $0.0004 / query

100K –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π √ó 1 GB –ø–∞–º—è—Ç–∏ –∫–∞–∂–¥—ã–π = 100 TB
‚Üí $9,600/–º–µ—Å—è—Ü —Ç–æ–ª—å–∫–æ –∑–∞ —Ö—Ä–∞–Ω–µ–Ω–∏–µ
‚Üí $$$
```

**–†–µ—à–µ–Ω–∏–µ:**
- **Compression** (—Å–∂–∞—Ç–∏–µ —Å—Ç–∞—Ä—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤)
- **Expiry** (—É–¥–∞–ª–µ–Ω–∏–µ –ø–æ—Å–ª–µ N –º–µ—Å—è—Ü–µ–≤)
- **Self-Hosted** (Chroma, Weaviate)

---

## ‚úÖ –ß–ï–ö–õ–ò–°–¢ –ü–û–ù–ò–ú–ê–ù–ò–Ø

- [ ] –ü–æ–Ω–∏–º–∞—é —Ä–∞–∑–Ω–∏—Ü—É Short-Term vs Long-Term Memory
- [ ] –ó–Ω–∞—é, —á—Ç–æ —Ç–∞–∫–æ–µ Context Window (4K-1M tokens)
- [ ] –ü–æ–Ω–∏–º–∞—é, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Vector Database
- [ ] –ú–æ–≥—É –Ω–∞–∑–≤–∞—Ç—å 3 Vector DB (Pinecone, Chroma, Weaviate)
- [ ] –ü–æ–Ω–∏–º–∞—é 3 —Ç–∏–ø–∞ –ø–∞–º—è—Ç–∏ (Episodic, Semantic, Procedural)
- [ ] –ó–Ω–∞—é, —á—Ç–æ —Ç–∞–∫–æ–µ Context Engineering

---

## üéØ –°–õ–ï–î–£–Æ–©–ò–ô –®–ê–ì

### –ß—Ç–æ –≤—ã —É–∑–Ω–∞–ª–∏:
- ‚úÖ –¢–∏–ø—ã –ø–∞–º—è—Ç–∏ (STM vs LTM)
- ‚úÖ Context Window (–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –∏ —Ä–µ—à–µ–Ω–∏—è)
- ‚úÖ Vector Databases (Pinecone, Chroma)
- ‚úÖ Context Engineering (—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º)

### –ü–æ—á–µ–º—É —Å–ª–µ–¥—É—é—â–∞—è –≥–ª–∞–≤–∞ –≤–∞–∂–Ω–∞:

**–ü—Ä–æ–±–ª–µ–º–∞:** –ê–≥–µ–Ω—Ç –ü–û–ú–ù–ò–¢ –∫–æ–Ω—Ç–µ–∫—Å—Ç, –Ω–æ –∫–∞–∫ –æ–Ω **–î–ï–ô–°–¢–í–£–ï–¢** –≤ –º–∏—Ä–µ? –ö–∞–∫ –≤—ã–∑—ã–≤–∞–µ—Ç API, —á–∏—Ç–∞–µ—Ç —Ñ–∞–π–ª—ã, –ø–∏—à–µ—Ç –∫–æ–¥?

**–û—Ç–≤–µ—Ç:** ‚Üí **–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∞–≥–µ–Ω—Ç–æ–≤** (Tools / Function Calling)

---

**‚Üí [08-–ò–ù–°–¢–†–£–ú–ï–ù–¢–´-–ê–ì–ï–ù–¢–û–í.md](08-–ò–ù–°–¢–†–£–ú–ï–ù–¢–´-–ê–ì–ï–ù–¢–û–í.md)** ‚Äî –û—Ç –ø–∞–º—è—Ç–∏ –∫ –¥–µ–π—Å—Ç–≤–∏—é

**–í—Ä–µ–º—è:** ~2 —á–∞—Å–∞ | **–°–ª–æ–∂–Ω–æ—Å—Ç—å:** üìäüìäüìäüìä‚ö™ (4/5)

---

**–í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –Ω–∞–≤–∏–≥–∞—Ü–∏–∏:** [00-–ù–ê–í–ò–ì–ê–¶–ò–Ø-–ü–û-–†–ê–ó–î–ï–õ–£.md](00-–ù–ê–í–ò–ì–ê–¶–ò–Ø-–ü–û-–†–ê–ó–î–ï–õ–£.md)

---

## üìö –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–´–ï –†–ï–°–£–†–°–´

### –°—Ç–∞—Ç—å–∏
- üìÑ **"Long-Context Language Models"** ‚Äî Stanford (2024)
- üìÑ **"Retrieval-Augmented Generation (RAG)"** ‚Äî Meta Research

### –í–∏–¥–µ–æ
- üé• **LangChain: Memory Systems** ‚Äî https://www.youtube.com/...
- üé• **Pinecone Tutorial** ‚Äî https://www.youtube.com/...

### –ö—É—Ä—Å—ã
- üéì **DeepLearning.AI: Building RAG Applications** (2024)

---

**–í—Ä–µ–º—è —á—Ç–µ–Ω–∏—è –≥–ª–∞–≤—ã:** ~2 —á–∞—Å–∞
**–°–ª–µ–¥—É—é—â–∞—è –≥–ª–∞–≤–∞:** [08-–ò–ù–°–¢–†–£–ú–ï–ù–¢–´-–ê–ì–ï–ù–¢–û–í.md](08-–ò–ù–°–¢–†–£–ú–ï–ù–¢–´-–ê–ì–ï–ù–¢–û–í.md)

---

## üß† –¢–ï–û–†–ò–Ø –°–ò–°–¢–ï–ú –ü–ê–ú–Ø–¢–ò –ê–ì–ï–ù–¢–û–í

### üìñ –ö–û–ù–¶–ï–ü–¢–£–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨ –ü–ê–ú–Ø–¢–ò

**–ü–∞–º—è—Ç—å AI-–∞–≥–µ–Ω—Ç–∞** ‚Äî —ç—Ç–æ –º–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –≤–¥–æ—Ö–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –∫–æ–≥–Ω–∏—Ç–∏–≤–Ω–æ–π –ø—Å–∏—Ö–æ–ª–æ–≥–∏–µ–π –∏ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ–¥ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π.

---

### 1Ô∏è‚É£ –¢–ò–ü–´ –ü–ê–ú–Ø–¢–ò –í AI-–ê–ì–ï–ù–¢–ê–•

#### –ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (Short-Term Memory, STM)

**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –í—Ä–µ–º–µ–Ω–Ω–æ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏.

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- **–†–∞–∑–º–µ—Ä:** –û–≥—Ä–∞–Ω–∏—á–µ–Ω —Ä–∞–∑–º–µ—Ä–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ –º–æ–¥–µ–ª–∏ (4K-2M tokens)
- **–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –°—É—â–µ—Å—Ç–≤—É–µ—Ç —Ç–æ–ª—å–∫–æ –≤ —Ä–∞–º–∫–∞—Ö —Ç–µ–∫—É—â–µ–π —Å–µ—Å—Å–∏–∏
- **–°–∫–æ—Ä–æ—Å—Ç—å –¥–æ—Å—Ç—É–ø–∞:** –ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–π (O(1) –ø–æ –≤—Ä–µ–º–µ–Ω–∏)
- **–§–æ—Ä–º–∞—Ç:** –°—ã—Ä–æ–π —Ç–µ–∫—Å—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ

**–ú–µ—Ö–∞–Ω–∏–∑–º —Ä–∞–±–æ—Ç—ã:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  –ö–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ (8K tokens)           ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  [System Prompt]        500 tokens      ‚îÇ
‚îÇ  [User Message 1]       200 tokens      ‚îÇ
‚îÇ  [Assistant Response]   300 tokens      ‚îÇ
‚îÇ  [User Message 2]       150 tokens      ‚îÇ
‚îÇ  ...                                    ‚îÇ
‚îÇ  [Current Processing]   ~1000 tokens    ‚îÇ
‚îÇ                                         ‚îÇ
‚îÇ  –î–æ—Å—Ç—É–ø–Ω–∞—è –ø–∞–º—è—Ç—å: 5,850 / 8,000 tokens ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞:**
- ‚úÖ –ù–µ —Ç—Ä–µ–±—É–µ—Ç –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
- ‚úÖ –ü–æ–ª–Ω—ã–π –∫–æ–Ω—Ç—Ä–æ–ª—å –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (–º–æ–¥–µ–ª—å "–≤–∏–¥–∏—Ç" –≤—Å—ë)
- ‚úÖ –ù—É–ª–µ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –Ω–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ

**–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è:**
- ‚ùå –§–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä (hard limit)
- ‚ùå –õ–∏–Ω–µ–π–Ω–æ–µ —É–≤–µ–ª–∏—á–µ–Ω–∏–µ —Å—Ç–æ–∏–º–æ—Å—Ç–∏ —Å —Ä–æ—Å—Ç–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- ‚ùå –ü–æ—Ç–µ—Ä—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø—Ä–∏ –ø–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–∏

---

#### –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å (Working Memory)

**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –ê–∫—Ç–∏–≤–Ω–æ–µ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –¥–ª—è —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏.

**–ö–æ–Ω—Ü–µ–ø—Ü–∏—è:**
```
–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (8K tokens)
    ‚îÇ
    ‚îî‚îÄ‚Üí –†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å (2K tokens)
        - –¢–µ–∫—É—â–∏–π –∑–∞–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        - –†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        - –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è
```

**–ü—Ä–∏–º–µ—Ä:**
```
–ó–∞–¥–∞—á–∞: "–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –∫–æ–¥ –∏ –Ω–∞–π–¥–∏ –±–∞–≥–∏"

–†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å –≤–∫–ª—é—á–∞–µ—Ç:
- –ö–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (500 tokens)
- –ü—Ä–∞–≤–∏–ª–∞ –ø–æ–∏—Å–∫–∞ –±–∞–≥–æ–≤ (300 tokens)
- –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ (200 tokens)
- –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ (1000 tokens)

–ù–µ –≤–∫–ª—é—á–∞–µ—Ç:
- –ü—Ä–µ–¥—ã–¥—É—â–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π –æ –¥—Ä—É–≥–∏—Ö —Ç–µ–º–∞—Ö
- System prompt (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, –Ω–æ –Ω–µ –≤ –∞–∫—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–µ)
```

**–ú–µ—Ö–∞–Ω–∏–∑–º:**
- **Attention Mechanism** —Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã –Ω–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π —á–∞—Å—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
- **Query-Key-Value** —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑–≤–ª–µ–∫–∞–µ—Ç –Ω—É–∂–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã

---

#### –î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (Long-Term Memory, LTM)

**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –í–Ω–µ—à–Ω–µ–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏, –¥–æ—Å—Ç—É–ø–Ω–æ–µ —á–µ—Ä–µ–∑ –º–µ—Ö–∞–Ω–∏–∑–º—ã –ø–æ–∏—Å–∫–∞ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (retrieval).

**–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏:**
- **–†–∞–∑–º–µ—Ä:** –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω (–∑–∞–≤–∏—Å–∏—Ç –æ—Ç –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã)
- **–î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:** –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞—è (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏)
- **–°–∫–æ—Ä–æ—Å—Ç—å –¥–æ—Å—Ç—É–ø–∞:** –ó–∞–≤–∏—Å–∏—Ç –æ—Ç —Å–∏—Å—Ç–µ–º—ã –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ (10-100ms –¥–ª—è vector search)
- **–§–æ—Ä–º–∞—Ç:** Embeddings (–≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è)

**–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–µ–Ω–∏—è:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Vector Database (Pinecone/Chroma)       ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ  Document 1:                             ‚îÇ
‚îÇ    - Text: "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å Python dev"    ‚îÇ
‚îÇ    - Embedding: [0.23, -0.56, ...]      ‚îÇ
‚îÇ    - Metadata: {timestamp, user_id}     ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ  Document 2:                             ‚îÇ
‚îÇ    - Text: "–†–∞–±–æ—Ç–∞–µ—Ç –≤ AI —Å—Ç–∞—Ä—Ç–∞–ø–µ"     ‚îÇ
‚îÇ    - Embedding: [0.18, -0.43, ...]      ‚îÇ
‚îÇ    - Metadata: {timestamp, user_id}     ‚îÇ
‚îÇ                                          ‚îÇ
‚îÇ  ... (–º–∏–ª–ª–∏–æ–Ω—ã –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**–ü—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (Retrieval):**
```
1. Query: "–ß—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≥–æ–≤–æ—Ä–∏–ª –ø—Ä–æ —Å–≤–æ—é —Ä–∞–±–æ—Ç—É?"
   ‚Üì
2. Query Embedding: [0.21, -0.48, ...] (384-1536 dimensions)
   ‚Üì
3. Vector Similarity Search (Cosine/Euclidean):
   distance(query_emb, doc_emb) ‚Üí Top-K results
   ‚Üì
4. Retrieved Documents:
   - Doc 2: "–†–∞–±–æ—Ç–∞–µ—Ç –≤ AI —Å—Ç–∞—Ä—Ç–∞–ø–µ" (similarity: 0.87)
   - Doc 15: "–ò–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç—Å—è ML" (similarity: 0.82)
   ‚Üì
5. Add to Context Window:
   [Query] + [Retrieved Docs] ‚Üí LLM ‚Üí Answer
```

---

### 2Ô∏è‚É£ –ö–ê–ö –ê–ì–ï–ù–¢–´ –ó–ê–ü–û–ú–ò–ù–ê–Æ–¢ –ò –ò–ó–í–õ–ï–ö–ê–Æ–¢ –ò–ù–§–û–†–ú–ê–¶–ò–Æ

#### –ü—Ä–æ—Ü–µ—Å—Å –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è (Encoding)

**1. Semantic Chunking (–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ)**

–î–ª–∏–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç ‚Üí –Ω–µ–±–æ–ª—å—à–∏–µ —Å–º—ã—Å–ª–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã:

```python
# –ü–ª–æ—Ö–æ–π –ø–æ–¥—Ö–æ–¥: —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä
chunks = split_text(text, chunk_size=500)  # –ú–æ–∂–µ—Ç —Ä–∞–∑–æ—Ä–≤–∞—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

# –•–æ—Ä–æ—à–∏–π –ø–æ–¥—Ö–æ–¥: —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Ä–∞–∑–±–∏–µ–Ω–∏–µ
chunks = semantic_chunking(text, model="sentence-transformer")
# ‚Üí ["–ê–±–∑–∞—Ü 1", "–ê–±–∑–∞—Ü 2", "–°–ø–∏—Å–æ–∫ –ø—É–Ω–∫—Ç–æ–≤ 1-3", ...]
```

**2. Embedding Generation (–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π)**

```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')

text = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º"
embedding = model.encode(text)  # ‚Üí [0.23, -0.56, 0.89, ...] (384 dims)
```

**3. Metadata Enrichment (–û–±–æ–≥–∞—â–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏)**

```python
document = {
    "text": text,
    "embedding": embedding.tolist(),
    "metadata": {
        "timestamp": "2024-03-15T14:30:00Z",
        "user_id": "user_123",
        "session_id": "session_456",
        "memory_type": "semantic",  # episodic/semantic/procedural
        "importance": 0.8,  # 0-1 scale
        "source": "chat_message"
    }
}

vector_db.upsert([document])
```

---

#### –ü—Ä–æ—Ü–µ—Å—Å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è (Retrieval)

**1. Hybrid Search (–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫)**

–ö–æ–º–±–∏–Ω–∞—Ü–∏—è **semantic search** (–≤–µ–∫—Ç–æ—Ä–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ) –∏ **keyword search** (—Ç–µ–∫—Å—Ç–æ–≤—ã–π –ø–æ–∏—Å–∫):

```python
# Semantic Search
query_embedding = model.encode("–ö–∞–∫–æ–π —É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã?")
semantic_results = vector_db.query(
    vector=query_embedding,
    top_k=5,
    filter={"user_id": "user_123"}  # –ò–∑–æ–ª—è—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
)

# Keyword Search (BM25)
keyword_results = full_text_search("–æ–ø—ã—Ç —Ä–∞–±–æ—Ç—ã —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫")

# Hybrid Fusion
results = rerank(semantic_results + keyword_results)
```

**2. Contextual Reranking (–ü–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏)**

```python
# –ü—Ä–æ–±–ª–µ–º–∞: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –ø–æ—Ö–æ–∂–∏–µ, –Ω–æ –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
# –†–µ—à–µ–Ω–∏–µ: reranking –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

from sentence_transformers import CrossEncoder

reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')

# –ü–∞—Ä—ã: (query, candidate_text)
pairs = [(query, result.text) for result in results]
scores = reranker.predict(pairs)

# –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –Ω–æ–≤—ã–º —Å–∫–æ—Ä–∞–º
reranked = sorted(zip(results, scores), key=lambda x: x[1], reverse=True)
```

**3. Memory Consolidation (–ö–æ–Ω—Å–æ–ª–∏–¥–∞—Ü–∏—è –ø–∞–º—è—Ç–∏)**

–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ö–æ–∂–∏—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π:

```python
# –ö–∞–∂–¥—ã–µ 100 —Å–æ–æ–±—â–µ–Ω–∏–π:
# 1. –ù–∞–π—Ç–∏ –ø–æ—Ö–æ–∂–∏–µ embeddings (cluster analysis)
similar_memories = find_clusters(embeddings, threshold=0.9)

# 2. –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –≤ summary
for cluster in similar_memories:
    summary = llm.summarize([mem.text for mem in cluster])
    new_memory = {
        "text": summary,
        "embedding": model.encode(summary),
        "metadata": {
            "type": "consolidated",
            "source_ids": [mem.id for mem in cluster],
            "importance": max([mem.importance for mem in cluster])
        }
    }
    vector_db.upsert([new_memory])
    # 3. –£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—ã–µ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
    vector_db.delete([mem.id for mem in cluster])
```

---

### 3Ô∏è‚É£ RAG –ò VECTOR DATABASES

#### RAG (Retrieval-Augmented Generation)

**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:** –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω, –≥–¥–µ LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ **–∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö** –≤–Ω–µ—à–Ω–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

**–§–æ—Ä–º—É–ª–∞:**
```
Answer = LLM(Query + Retrieved_Context)
```

**Pipeline:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  1. USER QUERY                                     ‚îÇ
‚îÇ     "–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ API –≤ –ø—Ä–æ–µ–∫—Ç–µ X?"          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  2. QUERY PROCESSING                               ‚îÇ
‚îÇ     - Intent classification: 'information_retrieval'‚îÇ
‚îÇ     - Entity extraction: ['API', '–ø—Ä–æ–µ–∫—Ç X']       ‚îÇ
‚îÇ     - Query expansion: + synonyms                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  3. EMBEDDING                                      ‚îÇ
‚îÇ     Query ‚Üí [0.12, -0.34, 0.78, ...] (384 dims)    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  4. VECTOR SEARCH                                  ‚îÇ
‚îÇ     Vector DB: Pinecone/Chroma/Weaviate            ‚îÇ
‚îÇ     - Index: HNSW/IVF/Flat                         ‚îÇ
‚îÇ     - Metric: Cosine similarity                    ‚îÇ
‚îÇ     - Top-K: 5 results                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  5. RETRIEVED DOCUMENTS                            ‚îÇ
‚îÇ     Doc 1: "API –¥–æ–ª–∂–µ–Ω –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å REST" (0.89)   ‚îÇ
‚îÇ     Doc 2: "–í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —á–µ—Ä–µ–∑ /v1/" (0.85)    ‚îÇ
‚îÇ     Doc 3: "–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è JWT" (0.82)            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  6. CONTEXT CONSTRUCTION                           ‚îÇ
‚îÇ     Context = f"""                                 ‚îÇ
‚îÇ     –î–æ–∫—É–º–µ–Ω—Ç—ã –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:                      ‚îÇ
‚îÇ     {doc1.text}                                    ‚îÇ
‚îÇ     {doc2.text}                                    ‚îÇ
‚îÇ     {doc3.text}                                    ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ     –í–æ–ø—Ä–æ—Å: {query}                                ‚îÇ
‚îÇ     """                                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  7. LLM GENERATION                                 ‚îÇ
‚îÇ     GPT-4/Claude: Context ‚Üí Answer                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  8. ANSWER + CITATIONS                             ‚îÇ
‚îÇ     "–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ API:                             ‚îÇ
‚îÇ     1. REST –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ [Doc 1]                    ‚îÇ
‚îÇ     2. –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ /v1/ [Doc 2]                ‚îÇ
‚îÇ     3. JWT –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è [Doc 3]"                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

#### Vector Databases: –≥–ª—É–±–æ–∫–æ–µ –ø–æ–≥—Ä—É–∂–µ–Ω–∏–µ

**1. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è (HNSW ‚Äî Hierarchical Navigable Small World)**

```
–ü—Ä–æ–±–ª–µ–º–∞: Linear search O(N) —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π –¥–ª—è –º–∏–ª–ª–∏–æ–Ω–æ–≤ –≤–µ–∫—Ç–æ—Ä–æ–≤

–†–µ—à–µ–Ω–∏–µ: HNSW ‚Äî –≥—Ä–∞—Ñ –ø—Ä–∏–±–ª–∏–∂—ë–Ω–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

–°—Ç—Ä—É–∫—Ç—É—Ä–∞:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Layer 2 (sparse, long edges)      ‚îÇ
‚îÇ    ‚Ä¢ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí ‚Ä¢           ‚îÇ
‚îÇ                                    ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 1 (medium density)          ‚îÇ
‚îÇ    ‚Ä¢ ‚îÄ‚îÄ‚Üí ‚Ä¢ ‚îÄ‚îÄ‚Üí ‚Ä¢                   ‚îÇ
‚îÇ     ‚Üì     ‚Üì     ‚Üì                  ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  Layer 0 (dense, all vectors)      ‚îÇ
‚îÇ    ‚Ä¢ ‚Üí ‚Ä¢ ‚Üí ‚Ä¢ ‚Üí ‚Ä¢ ‚Üí ‚Ä¢ ‚Üí ‚Ä¢           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

–ü–æ–∏—Å–∫:
1. Start at top layer ‚Üí greedy search to closest node
2. Move down ‚Üí refine search
3. Layer 0 ‚Üí exact neighbors

–°–ª–æ–∂–Ω–æ—Å—Ç—å: O(log N) –≤–º–µ—Å—Ç–æ O(N)
```

**2. –ú–µ—Ç—Ä–∏–∫–∏ —Å—Ö–æ–¥—Å—Ç–≤–∞**

| –ú–µ—Ç—Ä–∏–∫–∞ | –§–æ—Ä–º—É–ª–∞ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å |
|---------|---------|-------------------|
| **Cosine** | `cos(Œ∏) = (A¬∑B) / (‚ÄñA‚Äñ‚ÄñB‚Äñ)` | –¢–µ–∫—Å—Ç, —Å–µ–º–∞–Ω—Ç–∏–∫–∞ (direction –≤–∞–∂–Ω–µ–µ magnitude) |
| **Euclidean** | `‚àöŒ£(ai - bi)¬≤` | Embeddings –æ—Ç –º–æ–¥–µ–ª–µ–π —Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–µ–π |
| **Dot Product** | `Œ£(ai √ó bi)` | –£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è Cosine –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ |

**3. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Vector DB**

| –ë–∞–∑–∞ | –¢–∏–ø | –ò–Ω–¥–µ–∫—Å | –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã | –¶–µ–Ω–∞ |
|------|-----|--------|-------|--------|------|
| **Pinecone** | Cloud | HNSW | Managed, –±—ã—Å—Ç—Ä—ã–π | Vendor lock-in | $70+/–º–µ—Å |
| **Chroma** | Open-source | HNSW | –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π, –ø—Ä–æ—Å—Ç–æ–π | Self-hosted | $0 (—Ö–æ—Å—Ç–∏–Ω–≥ —Å–≤–æ–π) |
| **Weaviate** | –ì–∏–±—Ä–∏–¥ | HNSW | –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ | –°–ª–æ–∂–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ | $0-200/–º–µ—Å |
| **Qdrant** | Open-source | HNSW | Rust (–±—ã—Å—Ç—Ä–æ), —Ñ–∏–ª—å—Ç—Ä—ã | –ú–µ–Ω—å—à–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–π | $0 (self-host) |
| **Milvus** | Open-source | IVF/HNSW | –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π | –°–ª–æ–∂–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞ | $0 (self-host) |

---

### 4Ô∏è‚É£ –ü–†–û–ë–õ–ï–ú–´ –ü–ê–ú–Ø–¢–ò –ê–ì–ï–ù–¢–û–í

#### 1. –ó–∞–±—ã–≤–∞–Ω–∏–µ (Catastrophic Forgetting)

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
–î–µ–Ω—å 1: –ê–≥–µ–Ω—Ç –ø–æ–º–Ω–∏—Ç, —á—Ç–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –ª—é–±–∏—Ç Python
–î–µ–Ω—å 30: 1000+ –Ω–æ–≤—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
‚Üí –°—Ç–∞—Ä–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤—ã—Ç–µ—Å–Ω–µ–Ω–∞ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
‚Üí –ù–µ –∏–∑–≤–ª–µ—á–µ–Ω–∞ –∏–∑ Vector DB (query –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–µ–Ω)
‚Üí –ó–ê–ë–´–¢–û
```

**–†–µ—à–µ–Ω–∏—è:**

**–∞) Importance Scoring (–û—Ü–µ–Ω–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏)**
```python
def calculate_importance(text, metadata):
    score = 0.5  # base

    # –Ø–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
    if "–≤–∞–∂–Ω–æ" in text.lower():
        score += 0.3

    # –õ–∏—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    if any(word in text for word in ["–ª—é–±–ª—é", "—Ä–∞–±–æ—Ç–∞—é", "–∂–∏–≤—É"]):
        score += 0.2

    # Frequency (–∫–∞–∫ —á–∞—Å—Ç–æ —É–ø–æ–º–∏–Ω–∞–ª–æ—Å—å)
    score += min(metadata.get('mention_count', 0) * 0.05, 0.3)

    return min(score, 1.0)

# –ü—Ä–∏ retrieval: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤—ã—Å–æ–∫–∏–º scores
results = vector_db.query(
    vector=query_emb,
    filter={"importance": {"$gte": 0.7}}  # –¢–æ–ª—å–∫–æ –≤–∞–∂–Ω—ã–µ
)
```

**–±) Periodic Review (–ü–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–æ–µ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ)**
```python
# –ö–∞–∂–¥—ã–µ 7 –¥–Ω–µ–π: "review" —Å—Ç–∞—Ä–æ–π –≤–∞–∂–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
old_important = vector_db.query(
    filter={
        "importance": {"$gte": 0.8},
        "last_accessed": {"$lt": "7_days_ago"}
    }
)

# –î–æ–±–∞–≤–∏—Ç—å –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–µ–¥—É—é—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
system_prompt += f"\n–í–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –ø—Ä–æ—à–ª–æ–≥–æ:\n{old_important}"
```

**–≤) Decay Function (–ó–∞–±—ã–≤–∞–Ω–∏–µ —Å–æ –≤—Ä–µ–º–µ–Ω–µ–º)**
```python
import math
from datetime import datetime, timedelta

def decay_importance(original_importance, timestamp):
    days_old = (datetime.now() - timestamp).days
    # Exponential decay: half-life = 30 days
    decay_factor = math.exp(-days_old / 30)
    return original_importance * decay_factor

# –ü—Ä–∏ retrieval —É—á–∏—Ç—ã–≤–∞—Ç—å decay
results_with_decay = [
    (doc, doc.importance * decay_factor(doc.timestamp))
    for doc in results
]
```

---

#### 2. –ö–æ–Ω—Ñ–ª–∏–∫—Ç—ã –ø–∞–º—è—Ç–∏ (Memory Conflicts)

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
–°–æ–æ–±—â–µ–Ω–∏–µ 10: "–Ø —Ä–∞–±–æ—Ç–∞—é Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º"
–°–æ–æ–±—â–µ–Ω–∏–µ 500: "–Ø –ø–µ—Ä–µ—à—ë–ª –Ω–∞ Go —Ä–∞–∑—Ä–∞–±–æ—Ç–∫—É"

Query: "–ö–∞–∫–æ–π —è–∑—ã–∫ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å?"
‚Üí Retrieval –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –û–ë–ê –¥–æ–∫—É–º–µ–Ω—Ç–∞
‚Üí –ö–æ–Ω—Ñ–ª–∏–∫—Ç!
```

**–†–µ—à–µ–Ω–∏—è:**

**–∞) Temporal Ordering (–í—Ä–µ–º–µ–Ω–Ω–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)**
```python
# –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º –ø—Ä–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–µ
results = vector_db.query(vector=query_emb, top_k=10)

# –ì—Ä—É–ø–ø–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–µ–º–µ
topics = group_by_topic(results)  # ML clustering

# –î–ª—è –∫–∞–∂–¥–æ–π —Ç–µ–º—ã: –≤—ã–±—Ä–∞—Ç—å —Å–∞–º—ã–π —Å–≤–µ–∂–∏–π
resolved = [max(cluster, key=lambda x: x.timestamp) for cluster in topics]
```

**–±) Explicit Updates (–Ø–≤–Ω–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ)**
```python
# –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è
if detect_contradiction(new_info, existing_memories):
    # –ü–æ–º–µ—Ç–∏—Ç—å —Å—Ç–∞—Ä–æ–µ –∫–∞–∫ —É—Å—Ç–∞—Ä–µ–≤—à–µ–µ
    vector_db.update(
        id=old_memory.id,
        metadata={"deprecated": True, "superseded_by": new_memory.id}
    )

    # –°–æ–∑–¥–∞—Ç—å –Ω–æ–≤–æ–µ
    vector_db.upsert(new_memory)

# –ü—Ä–∏ retrieval: —Ñ–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å deprecated
results = vector_db.query(
    filter={"deprecated": {"$ne": True}}
)
```

**–≤) Confidence Scoring (–û—Ü–µ–Ω–∫–∞ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏)**
```python
memory_1 = {"text": "–†–∞–±–æ—Ç–∞—é –Ω–∞ Python", "confidence": 0.9}
memory_2 = {"text": "–ü–µ—Ä–µ—à—ë–ª –Ω–∞ Go", "confidence": 0.95}

# LLM —Ä–∞–∑—Ä–µ—à–∞–µ—Ç –∫–æ–Ω—Ñ–ª–∏–∫—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ confidence
resolution = llm.resolve_conflict(memory_1, memory_2)
# ‚Üí "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ä–∞–Ω—å—à–µ —Ä–∞–±–æ—Ç–∞–ª –Ω–∞ Python, —Ç–µ–ø–µ—Ä—å –Ω–∞ Go"
```

---

#### 3. –ü–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ (Memory Overflow)

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
10K –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π √ó 1000 —Å–æ–æ–±—â–µ–Ω–∏–π –∫–∞–∂–¥—ã–π = 10M –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
Embeddings: 384 dims √ó 4 bytes = 1.5 KB / –¥–æ–∫—É–º–µ–Ω—Ç
‚Üí 10M √ó 1.5 KB = 15 GB –ø–∞–º—è—Ç–∏
‚Üí Pinecone: $144/–º–µ—Å—è—Ü —Ç–æ–ª—å–∫–æ –∑–∞ —Ö—Ä–∞–Ω–µ–Ω–∏–µ
```

**–†–µ—à–µ–Ω–∏—è:**

**–∞) Compression (–°–∂–∞—Ç–∏–µ)**
```python
# Quantization: float32 ‚Üí int8
embeddings_int8 = (embeddings * 127).astype(np.int8)
# –†–∞–∑–º–µ—Ä: 384 dims √ó 1 byte = 384 bytes (4x –º–µ–Ω—å—à–µ!)

# Dimensionality Reduction: PCA/UMAP
from sklearn.decomposition import PCA
pca = PCA(n_components=128)  # 384 ‚Üí 128 dims
compressed = pca.fit_transform(embeddings)
# –†–∞–∑–º–µ—Ä: 3x –º–µ–Ω—å—à–µ (—Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –ø–æ—Ç–µ—Ä–µ–π —Ç–æ—á–Ω–æ—Å—Ç–∏)
```

**–±) Tiered Storage (–ú–Ω–æ–≥–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —Ö—Ä–∞–Ω–µ–Ω–∏–µ)**
```python
# Hot tier (—á–∞—Å—Ç—ã–π –¥–æ—Å—Ç—É–ø): Pinecone (–±—ã—Å—Ç—Ä–æ, –¥–æ—Ä–æ–≥–æ)
# Cold tier (—Ä–µ–¥–∫–∏–π –¥–æ—Å—Ç—É–ø): S3 + DuckDB (–º–µ–¥–ª–µ–Ω–Ω–æ, –¥–µ—à–µ–≤–æ)

if memory.access_count > 10:
    pinecone.upsert(memory)  # $$$
else:
    s3.upload(memory)  # $

# –ü—Ä–∏ retrieval:
results_hot = pinecone.query(...)
if len(results_hot) < top_k:
    results_cold = s3_search(...)  # Fallback
```

**–≤) Eviction Policy (–ü–æ–ª–∏—Ç–∏–∫–∞ —É–¥–∞–ª–µ–Ω–∏—è)**
```python
# LRU (Least Recently Used)
if memory_usage > threshold:
    old_memories = vector_db.query(
        order_by="last_accessed",
        limit=1000
    )
    vector_db.delete([m.id for m in old_memories])

# LFU (Least Frequently Used)
if memory_usage > threshold:
    rare_memories = vector_db.query(
        order_by="access_count",
        limit=1000
    )
    vector_db.delete([m.id for m in rare_memories])
```

---

#### 4. Hallucinated Memories (–õ–æ–∂–Ω—ã–µ –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏—è)

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
User: "–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ –Ω–∞—à—É –≤—Å—Ç—Ä–µ—á—É –Ω–∞ –∫–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏"
Agent: "–î–∞, –º—ã –æ–±—Å—É–∂–¥–∞–ª–∏ –±–ª–æ–∫—á–µ–π–Ω –∏ –∫–≤–∞–Ω—Ç–æ–≤—ã–µ –∫–æ–º–ø—å—é—Ç–µ—Ä—ã!"
‚Üí –ù–æ –≤—Å—Ç—Ä–µ—á–∏ –ù–ï –ë–´–õ–û (hallucination)
```

**–ü—Ä–∏—á–∏–Ω—ã:**
- LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç "–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–π" –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö
- Retrieval –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Ö–æ–∂–∏–µ, –Ω–æ –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
- –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤

**–†–µ—à–µ–Ω–∏—è:**

**–∞) Source Attribution (–ê—Ç—Ä–∏–±—É—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤)**
```python
# –í–°–ï–ì–î–ê —É–∫–∞–∑—ã–≤–∞—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫
answer = llm.generate(
    query=query,
    context=retrieved_docs,
    instruction="Cite sources for each claim using [Doc N] notation"
)

# –ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞:
# "–í—ã —Ä–∞–±–æ—Ç–∞–µ—Ç–µ Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–æ–º [Doc 5].
#  –ò–Ω—Ç–µ—Ä–µ—Å—É–µ—Ç–µ—Å—å ML [Doc 12]."
```

**–±) Confidence Thresholding (–ü–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏)**
```python
results = vector_db.query(vector=query_emb, top_k=5)

# –ï—Å–ª–∏ similarity < threshold ‚Üí "–ù–µ –∑–Ω–∞—é"
if results[0].score < 0.7:
    return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —è –Ω–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç—Ç–æ–º –≤ –Ω–∞—à–µ–π –∏—Å—Ç–æ—Ä–∏–∏."

# –ò–Ω–∞—á–µ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å retrieved context
return llm.generate(context=results)
```

**–≤) Explicit Memory Markers (–Ø–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã –ø–∞–º—è—Ç–∏)**
```python
# –°–æ—Ö—Ä–∞–Ω—è—Ç—å —Ç–æ–ª—å–∫–æ explicit statements
user_message = "–Ø —Ä–∞–±–æ—Ç–∞—é –≤ Google"

# –ü—Ä–æ–≤–µ—Ä–∫–∞: —ç—Ç–æ —Ñ–∞–∫—Ç –∏–ª–∏ –≥–∏–ø–æ—Ç–µ–∑–∞?
if is_factual_statement(user_message):
    vector_db.upsert({
        "text": user_message,
        "type": "explicit_fact",
        "verified": True
    })
else:
    # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –≥–∏–ø–æ—Ç–µ–∑—ã/–≤–æ–ø—Ä–æ—Å—ã –∫–∞–∫ —Ñ–∞–∫—Ç—ã
    pass
```

---

## ‚ùì –í–û–ü–†–û–°–´ –î–õ–Ø –°–ê–ú–û–ü–†–û–í–ï–†–ö–ò

### 1. –¢–∏–ø—ã –ø–∞–º—è—Ç–∏

**–í–æ–ø—Ä–æ—Å 1.1:** –í —á—ë–º —Ñ—É–Ω–¥–∞–º–µ–Ω—Ç–∞–ª—å–Ω–æ–µ —Ä–∞–∑–ª–∏—á–∏–µ –º–µ–∂–¥—É –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π (STM) –∏ –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π (LTM) –ø–∞–º—è—Ç—å—é –≤ AI-–∞–≥–µ–Ω—Ç–∞—Ö?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**–ö—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (STM):**
- –•—Ä–∞–Ω–∏—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–º –æ–∫–Ω–µ –º–æ–¥–µ–ª–∏ (4K-2M tokens)
- –ú–≥–Ω–æ–≤–µ–Ω–Ω—ã–π –¥–æ—Å—Ç—É–ø (O(1))
- –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–π —Ä–∞–∑–º–µ—Ä (hard limit)
- –ò—Å—á–µ–∑–∞–µ—Ç –ø–æ—Å–ª–µ —Å–µ—Å—Å–∏–∏
- –§–æ—Ä–º–∞—Ç: —Å—ã—Ä–æ–π —Ç–µ–∫—Å—Ç

**–î–æ–ª–≥–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å (LTM):**
- –•—Ä–∞–Ω–∏—Ç—Å—è –≤ Vector Database (–ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ)
- –î–æ—Å—Ç—É–ø —á–µ—Ä–µ–∑ retrieval (10-100ms)
- –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π —Ä–∞–∑–º–µ—Ä
- –ü–µ—Ä—Å–∏—Å—Ç–µ–Ω—Ç–Ω–∞—è (—Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏)
- –§–æ—Ä–º–∞—Ç: embeddings (–≤–µ–∫—Ç–æ—Ä—ã)

**–ö–ª—é—á–µ–≤–æ–µ —Ä–∞–∑–ª–∏—á–∏–µ:** STM ‚Äî —ç—Ç–æ "—á—Ç–æ –º–æ–¥–µ–ª—å –≤–∏–¥–∏—Ç —Å–µ–π—á–∞—Å", LTM ‚Äî —ç—Ç–æ "—á—Ç–æ –º–æ–¥–µ–ª—å –º–æ–∂–µ—Ç –Ω–∞–π—Ç–∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏".
</details>

---

**–í–æ–ø—Ä–æ—Å 1.2:** –û–±—ä—è—Å–Ω–∏—Ç–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏—é "—Ä–∞–±–æ—á–µ–π –ø–∞–º—è—Ç–∏" (Working Memory) –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ Transformer-–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã. –ö–∞–∫ –º–µ—Ö–∞–Ω–∏–∑–º attention —Ä–µ–∞–ª–∏–∑—É–µ—Ç —ç—Ç—É –∫–æ–Ω—Ü–µ–ø—Ü–∏—é?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**–†–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å** ‚Äî —ç—Ç–æ –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–æ –∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–æ–π –ø–∞–º—è—Ç–∏, –∞–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º–æ–µ –¥–ª—è —Ç–µ–∫—É—â–µ–π –∑–∞–¥–∞—á–∏.

**–†–µ–∞–ª–∏–∑–∞—Ü–∏—è —á–µ—Ä–µ–∑ Attention:**
```
Input tokens: [T1, T2, T3, ..., T100]
Query (—Ç–µ–∫—É—â–∏–π —Ç–æ–∫–µ–Ω): TN

Attention scores:
- T1: 0.05 (–Ω–∏–∑–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å)
- T2: 0.02
- T50: 0.45 (–≤—ã—Å–æ–∫–∞—è —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å!)
- T51: 0.38
- TN-1: 0.10

‚Üí –ú–æ–¥–µ–ª—å "—Ñ–æ–∫—É—Å–∏—Ä—É–µ—Ç—Å—è" –Ω–∞ T50, T51 (—Ä–∞–±–æ—á–∞—è –ø–∞–º—è—Ç—å)
‚Üí –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã "–≤ —Ñ–æ–Ω–µ" (–∫—Ä–∞—Ç–∫–æ—Å—Ä–æ—á–Ω–∞—è –ø–∞–º—è—Ç—å)
```

**–ú–µ—Ö–∞–Ω–∏–∑–º:**
1. **Query-Key matching** –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞
2. **Softmax** –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç scores (—Å—É–º–º–∞ = 1)
3. **Value weighting** –≤–∑–≤–µ—à–∏–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ scores
4. –†–µ–∑—É–ª—å—Ç–∞—Ç: –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ **—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π** –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏

**–ê–Ω–∞–ª–æ–≥–∏—è:** –ß–µ–ª–æ–≤–µ–∫ —á–∏—Ç–∞–µ—Ç –∫–Ω–∏–≥—É. –í—Å—è –∫–Ω–∏–≥–∞ –≤ –ø–∞–º—è—Ç–∏ (STM), –Ω–æ –∞–∫—Ç–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ —Ç–µ–∫—É—â–∏–π –∞–±–∑–∞—Ü (Working Memory).
</details>

---

### 2. –ó–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ

**–í–æ–ø—Ä–æ—Å 2.1:** –û–ø–∏—à–∏—Ç–µ –ø–æ–ª–Ω—ã–π pipeline –ø—Ä–æ—Ü–µ—Å—Å–∞ –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –≤ Vector Database. –ö–∞–∫–∏–µ —ç—Ç–∞–ø—ã –∫—Ä–∏—Ç–∏—á–Ω—ã –¥–ª—è –∫–∞—á–µ—Å—Ç–≤–∞ retrieval?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**Pipeline –∑–∞–ø–æ–º–∏–Ω–∞–Ω–∏—è:**

1. **Chunking (–†–∞–∑–±–∏–µ–Ω–∏–µ):**
   - –¢–µ–∫—Å—Ç ‚Üí —Å–º—ã—Å–ª–æ–≤—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã (sentences/paragraphs)
   - –ö—Ä–∏—Ç–∏—á–Ω–æ: –Ω–µ —Ä–∞–∑—Ä—ã–≤–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç (–ø–ª–æ—Ö–æ: "Python —Ö–æ—Ä–æ—à" / "–¥–ª—è ML")

2. **Embedding Generation:**
   - –¢–µ–∫—Å—Ç ‚Üí –≤–µ–∫—Ç–æ—Ä (384-1536 dims)
   - –ö—Ä–∏—Ç–∏—á–Ω–æ: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç—É –∂–µ –º–æ–¥–µ–ª—å, —á—Ç–æ –¥–ª—è query

3. **Metadata Enrichment:**
   - timestamp, user_id, importance, type
   - –ö—Ä–∏—Ç–∏—á–Ω–æ: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –¥–ª—è –∏–∑–æ–ª—è—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π

4. **Indexing:**
   - –í—Å—Ç–∞–≤–∫–∞ –≤ Vector DB (HNSW/IVF index)
   - –ö—Ä–∏—Ç–∏—á–Ω–æ: –≤—ã–±–æ—Ä –º–µ—Ç—Ä–∏–∫–∏ (cosine –¥–ª—è —Ç–µ–∫—Å—Ç–∞)

5. **Verification:**
   - –ü—Ä–æ–≤–µ—Ä–∫–∞: –∑–∞–ø—Ä–æ—Å ‚Üí –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ —á—Ç–æ –¥–æ–±–∞–≤–ª–µ–Ω–Ω–æ–≥–æ
   - –ö—Ä–∏—Ç–∏—á–Ω–æ: –≤—ã—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏

**–ö—Ä–∏—Ç–∏—á–Ω—ã–µ —ç—Ç–∞–ø—ã –¥–ª—è quality retrieval:**
- **Chunking:** –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≥—Ä–∞–Ω—É–ª—è—Ä–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞
- **Embedding model:** –Ω–∞–ø—Ä—è–º—É—é –≤–ª–∏—è–µ—Ç –Ω–∞ semantic similarity
- **Metadata:** –ø–æ–∑–≤–æ–ª—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—é –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É–∞–ª–∏–∑–∞—Ü–∏—é
</details>

---

**–í–æ–ø—Ä–æ—Å 2.2:** –ß—Ç–æ —Ç–∞–∫–æ–µ "–≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫" (Hybrid Search)? –ü–æ—á–µ–º—É –æ–Ω –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —á–∏—Å—Ç—ã–π semantic search –¥–ª—è –º–Ω–æ–≥–∏—Ö –∑–∞–¥–∞—á?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**–ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫** = Semantic Search (–≤–µ–∫—Ç–æ—Ä—ã) + Keyword Search (BM25/TF-IDF)

**Semantic Search:**
- –ò—â–µ—Ç –ø–æ —Å–º—ã—Å–ª—É (synonyms, paraphrases)
- –ü—Ä–∏–º–µ—Ä: "–º–∞—à–∏–Ω–∞" –Ω–∞–π–¥—ë—Ç "–∞–≤—Ç–æ–º–æ–±–∏–ª—å", "–∞–≤—Ç–æ"
- –ü—Ä–æ–±–ª–µ–º–∞: –ø–ª–æ—Ö–æ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ç–æ—á–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏ ("GPT-4", "Python 3.12")

**Keyword Search:**
- –ò—â–µ—Ç —Ç–æ—á–Ω—ã–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è —Å–ª–æ–≤
- –ü—Ä–∏–º–µ—Ä: "Python 3.12" –Ω–∞–π–¥—ë—Ç —Ç–æ–ª—å–∫–æ "Python 3.12"
- –ü—Ä–æ–±–ª–µ–º–∞: –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç synonyms ("–º–∞—à–∏–Ω–∞" –ù–ï –Ω–∞–π–¥—ë—Ç "–∞–≤—Ç–æ–º–æ–±–∏–ª—å")

**Hybrid:**
```python
# –ü—Ä–∏–º–µ—Ä: "–ü—Ä–æ–±–ª–µ–º—ã —Å —É—Å—Ç–∞–Ω–æ–≤–∫–æ–π Python 3.12 –Ω–∞ MacOS"

# Semantic Search –Ω–∞–π–¥—ë—Ç:
- "–û—à–∏–±–∫–∏ –ø—Ä–∏ —É—Å—Ç–∞–Ω–æ–≤–∫–µ Python –Ω–∞ Mac" (synonyms)
- "–ö–∞–∫ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å Python –Ω–∞ MacOS" (paraphrase)

# Keyword Search –Ω–∞–π–¥—ë—Ç:
- –¢–æ—á–Ω–æ "Python 3.12" (–Ω–µ 3.11, –Ω–µ 3.10)
- –¢–æ—á–Ω–æ "MacOS" (–Ω–µ Windows, –Ω–µ Linux)

# Hybrid = –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ/–æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ —Å reranking
```

**–ü–æ—á–µ–º—É –ª—É—á—à–µ:**
- ‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å keywords + –≥–∏–±–∫–æ—Å—Ç—å semantics
- ‚úÖ –ú–µ–Ω—å—à–µ false positives (semantic) –∏ false negatives (keywords)
- ‚úÖ 10-20% —É–ª—É—á—à–µ–Ω–∏–µ recall –≤ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö
</details>

---

### 3. RAG –∏ Vector Databases

**–í–æ–ø—Ä–æ—Å 3.1:** –û–±—ä—è—Å–Ω–∏—Ç–µ, –ø–æ—á–µ–º—É RAG (Retrieval-Augmented Generation) —Ä–µ—à–∞–µ—Ç –ø—Ä–æ–±–ª–µ–º—É "hallucinations" (–≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π) LLM. –ü—Ä–∏–≤–µ–¥–∏—Ç–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –ø—Ä–∏–º–µ—Ä.

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**–ü—Ä–æ–±–ª–µ–º–∞ –≥–∞–ª–ª—é—Ü–∏–Ω–∞—Ü–∏–π:**
```
User: "–ö–∞–∫–∏–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è –∫ API –≤ –Ω–∞—à–µ–º –ø—Ä–æ–µ–∫—Ç–µ?"
LLM (–±–µ–∑ RAG): "API –¥–æ–ª–∂–µ–Ω –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å GraphQL –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å real-time"
‚Üí HALLUCINATION (LLM –ø—Ä–∏–¥—É–º–∞–ª, —Ç.–∫. –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö)
```

**–†–µ—à–µ–Ω–∏–µ —á–µ—Ä–µ–∑ RAG:**
```
1. Query ‚Üí Vector DB ‚Üí Retrieve actual project docs:
   - Doc 1: "API spec: REST, –≤–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ /v1/"
   - Doc 2: "–ê—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è: JWT tokens"

2. Context = Query + Retrieved Docs ‚Üí LLM

3. LLM (—Å RAG): "–°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞:
   - REST –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ [Doc 1]
   - –í–µ—Ä—Å–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ /v1/ [Doc 1]
   - JWT –∞—É—Ç–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è [Doc 2]"
```

**–ü–æ—á–µ–º—É —Ä–∞–±–æ—Ç–∞–µ—Ç:**
- ‚úÖ LLM –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –æ—Å–Ω–æ–≤–µ **—Ä–µ–∞–ª—å–Ω—ã—Ö** –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∞ –Ω–µ "–∑–Ω–∞–Ω–∏–π" –∏–∑ pretraining
- ‚úÖ –ê—Ç—Ä–∏–±—É—Ü–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ ([Doc N]) –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Ñ–∞–∫—Ç—ã
- ‚úÖ –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –≤ DB ‚Üí LLM –º–æ–∂–µ—Ç –æ—Ç–≤–µ—Ç–∏—Ç—å "–ù–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é"

**–ü—Ä–∏–º–µ—Ä:**
- **–ë–µ–∑ RAG:** "Python 4.0 –≤—ã–π–¥–µ—Ç –≤ 2025" (hallucination)
- **–° RAG:** Retrieval –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—Ö–æ–¥–∏—Ç ‚Üí "–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ Python 4.0 –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π"
</details>

---

**–í–æ–ø—Ä–æ—Å 3.2:** –°—Ä–∞–≤–Ω–∏—Ç–µ HNSW –∏ IVF –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é –≤ Vector Databases. –í –∫–∞–∫–∏—Ö —Å—Ü–µ–Ω–∞—Ä–∏—è—Ö –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∞ –∫–∞–∂–¥–∞—è?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

| –ü–∞—Ä–∞–º–µ—Ç—Ä | HNSW | IVF |
|----------|------|-----|
| **–°—Ç—Ä—É–∫—Ç—É—Ä–∞** | –ì—Ä–∞—Ñ (–Ω–∞–≤–∏–≥–∞—Ü–∏—è –ø–æ —Ä—ë–±—Ä–∞–º) | –ö–ª–∞—Å—Ç–µ—Ä—ã (Voronoi –¥–∏–∞–≥—Ä–∞–º–º–∞) |
| **–ü–æ–∏—Å–∫** | Greedy graph traversal | –ë–ª–∏–∂–∞–π—à–∏–π –∫–ª–∞—Å—Ç–µ—Ä ‚Üí linear scan |
| **–°–ª–æ–∂–Ω–æ—Å—Ç—å** | O(log N) | O(‚àöN) –¥–ª—è N/‚àöN –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ |
| **–¢–æ—á–Ω–æ—Å—Ç—å** | –í—ã—Å–æ–∫–∞—è (99%+ recall) | –°—Ä–µ–¥–Ω—è—è (85-95% recall) |
| **–°–∫–æ—Ä–æ—Å—Ç—å** | –û—á–µ–Ω—å –±—ã—Å—Ç—Ä–∞—è | –ë—ã—Å—Ç—Ä–∞—è |
| **–ü–∞–º—è—Ç—å** | –í—ã—Å–æ–∫–∞—è (–≥—Ä–∞—Ñ + –≤–µ–∫—Ç–æ—Ä—ã) | –°—Ä–µ–¥–Ω—è—è (—Ü–µ–Ω—Ç—Ä–æ–∏–¥—ã + –≤–µ–∫—Ç–æ—Ä—ã) |
| **Insert cost** | –í—ã—Å–æ–∫–∞—è (–ø–µ—Ä–µ—Å—Ç—Ä–æ–π–∫–∞ –≥—Ä–∞—Ñ–∞) | –ù–∏–∑–∫–∞—è (–¥–æ–±–∞–≤–∏—Ç—å –≤ –∫–ª–∞—Å—Ç–µ—Ä) |

**HNSW –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∞ –∫–æ–≥–¥–∞:**
- ‚úÖ –ù—É–∂–Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ–∏—Å–∫–∞ (99%+ recall)
- ‚úÖ Dataset —Å—Ç–∞—Ç–∏—á–Ω—ã–π (—Ä–µ–¥–∫–∏–µ updates)
- ‚úÖ –ï—Å—Ç—å –ø–∞–º—è—Ç—å –¥–ª—è –≥—Ä–∞—Ñ–∞
- ‚úÖ –ü—Ä–∏–º–µ—Ä—ã: production search, recommendation systems

**IVF –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–∞ –∫–æ–≥–¥–∞:**
- ‚úÖ –ß–∞—Å—Ç—ã–µ inserts/updates (–¥–∏–Ω–∞–º–∏—á–Ω—ã–π dataset)
- ‚úÖ –û–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–∞—è –ø–∞–º—è—Ç—å
- ‚úÖ –î–æ–ø—É—Å—Ç–∏–º–∞ –Ω–µ–±–æ–ª—å—à–∞—è –ø–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏ (90% recall OK)
- ‚úÖ –ü—Ä–∏–º–µ—Ä—ã: real-time streaming, prototyping

**Hybrid:**
- –ù–µ–∫–æ—Ç–æ—Ä—ã–µ DB (Milvus) –∫–æ–º–±–∏–Ω–∏—Ä—É—é—Ç: IVF + HNSW –≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
</details>

---

### 4. –ü—Ä–æ–±–ª–µ–º—ã –ø–∞–º—è—Ç–∏

**–í–æ–ø—Ä–æ—Å 4.1:** –û–ø–∏—à–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—É "catastrophic forgetting" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ AI-–∞–≥–µ–Ω—Ç–æ–≤. –ü—Ä–µ–¥–ª–æ–∂–∏—Ç–µ —Ç—Ä–∏ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –ø–æ–¥—Ö–æ–¥–∞ –∫ —Ä–µ—à–µ–Ω–∏—é —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∫–æ–¥–∞.

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**–ü—Ä–æ–±–ª–µ–º–∞:**
–ê–≥–µ–Ω—Ç "–∑–∞–±—ã–≤–∞–µ—Ç" —Å—Ç–∞—Ä—É—é –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑-–∑–∞:
1. –í—ã—Ç–µ—Å–Ω–µ–Ω–∏—è –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–≥–æ –æ–∫–Ω–∞ (STM overflow)
2. –ù–∏–∑–∫–æ–≥–æ semantic similarity –ø—Ä–∏ retrieval (LTM miss)
3. –ü–µ—Ä–µ–∑–∞–ø–∏—Å–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏

**–ü–æ–¥—Ö–æ–¥ 1: Importance Scoring**
```python
def calculate_importance(text, metadata):
    score = 0.5

    # –Ø–≤–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã
    important_keywords = ["–≤–∞–∂–Ω–æ", "–∫—Ä–∏—Ç–∏—á–Ω–æ", "–≤—Å–µ–≥–¥–∞ –ø–æ–º–Ω–∏—Ç—å"]
    if any(kw in text.lower() for kw in important_keywords):
        score += 0.3

    # –õ–∏—á–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–≤—ã—Å–æ–∫–∞—è –≤–∞–∂–Ω–æ—Å—Ç—å)
    personal_markers = ["–º–µ–Ω—è –∑–æ–≤—É—Ç", "—è —Ä–∞–±–æ—Ç–∞—é", "–º–æ–π email"]
    if any(pm in text.lower() for pm in personal_markers):
        score += 0.4

    # –ß–∞—Å—Ç–æ—Ç–∞ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
    score += min(metadata.get('mention_count', 0) * 0.05, 0.2)

    return min(score, 1.0)

# –ü—Ä–∏ retrieval: boosting –ø–æ importance
results = vector_db.query(
    vector=query_emb,
    top_k=10,
    score_modifier=lambda doc: doc.similarity * doc.importance
)
```

**–ü–æ–¥—Ö–æ–¥ 2: Periodic Review (Spaced Repetition)**
```python
from datetime import datetime, timedelta

def schedule_review(memory_id, importance):
    # –ê–ª–≥–æ—Ä–∏—Ç–º: Ebbinghaus forgetting curve
    intervals = [1, 3, 7, 14, 30, 90]  # days
    next_review = datetime.now() + timedelta(
        days=intervals[min(memory.review_count, 5)]
    )

    db.update(memory_id, {
        "next_review": next_review,
        "review_count": memory.review_count + 1
    })

# Daily job: inject old important memories into context
def daily_review_job():
    due_memories = db.query(
        filter={"next_review": {"$lte": datetime.now()}}
    )

    for memory in due_memories:
        # Add to next conversation context
        context_buffer.add(memory.text)
        schedule_review(memory.id, memory.importance)
```

**–ü–æ–¥—Ö–æ–¥ 3: Hierarchical Summarization**
```python
# –ö–∞–∂–¥—ã–µ N —Å–æ–æ–±—â–µ–Ω–∏–π: —Å–æ–∑–¥–∞—Ç—å summary
def consolidate_memories(user_id, since_timestamp):
    old_memories = db.query(
        filter={
            "user_id": user_id,
            "timestamp": {"$gte": since_timestamp},
            "type": "episodic"
        }
    )

    # LLM summarization
    summary = llm.summarize(
        [m.text for m in old_memories],
        instruction="Extract key facts and preserve important details"
    )

    # Create consolidated memory
    consolidated = {
        "text": summary,
        "type": "semantic",  # episodic ‚Üí semantic
        "importance": max([m.importance for m in old_memories]),
        "source_ids": [m.id for m in old_memories]
    }

    db.upsert(consolidated)

    # Mark old memories as consolidated (–Ω–µ —É–¥–∞–ª—è—Ç—å!)
    for m in old_memories:
        db.update(m.id, {"consolidated": True})
```
</details>

---

**–í–æ–ø—Ä–æ—Å 4.2:** –ö–∞–∫ —Ä–µ—à–∏—Ç—å –ø—Ä–æ–±–ª–µ–º—É –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π? –ù–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–Ω–∞—á–∞–ª–∞ —Å–∫–∞–∑–∞–ª "—Ä–∞–±–æ—Ç–∞—é –Ω–∞ Python", —á–µ—Ä–µ–∑ –º–µ—Å—è—Ü "–ø–µ—Ä–µ—à—ë–ª –Ω–∞ Rust". –ö–∞–∫ –∞–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å "–ù–∞ –∫–∞–∫–æ–º —è–∑—ã–∫–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å?"

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**–°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: Temporal Ordering (–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –Ω–æ–≤–æ–º—É)**
```python
def resolve_conflict_temporal(query, conflicting_memories):
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ timestamp (–Ω–æ–≤–æ–µ ‚Üí —Å—Ç–∞—Ä–æ–µ)
    sorted_memories = sorted(
        conflicting_memories,
        key=lambda m: m.timestamp,
        reverse=True
    )

    # –§–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∞ —Å —É—á—ë—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏
    prompt = f"""
    –ò—Å—Ç–æ—Ä–∏—è –∏–∑–º–µ–Ω–µ–Ω–∏–π:
    - {sorted_memories[-1].timestamp}: {sorted_memories[-1].text}
    - {sorted_memories[0].timestamp}: {sorted_memories[0].text}

    –í–æ–ø—Ä–æ—Å: {query}

    –û—Ç–≤–µ—Ç—å, —É—á–∏—Ç—ã–≤–∞—è –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å.
    """

    return llm.generate(prompt)

# –†–µ–∑—É–ª—å—Ç–∞—Ç: "–†–∞–Ω—å—à–µ —Ä–∞–±–æ—Ç–∞–ª –Ω–∞ Python, —Ç–µ–ø–µ—Ä—å –Ω–∞ Rust"
```

**–°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: Explicit Update Detection**
```python
def detect_update(new_memory, existing_memories):
    # Keywords: "—Ç–µ–ø–µ—Ä—å", "–ø–µ—Ä–µ—à—ë–ª", "–∏–∑–º–µ–Ω–∏–ª–æ—Å—å"
    update_markers = ["—Ç–µ–ø–µ—Ä—å", "–ø–µ—Ä–µ—à—ë–ª", "–±–æ–ª—å—à–µ –Ω–µ", "–∏–∑–º–µ–Ω–∏–ª"]

    if any(marker in new_memory.text.lower() for marker in update_markers):
        # –≠—Ç–æ explicit update ‚Üí –ø–æ–º–µ—Ç–∏—Ç—å —Å—Ç–∞—Ä–æ–µ –∫–∞–∫ deprecated
        for old in existing_memories:
            if semantic_similar(new_memory, old) > 0.8:
                db.update(old.id, {
                    "deprecated": True,
                    "superseded_by": new_memory.id,
                    "deprecated_at": datetime.now()
                })

        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ–≤–æ–µ —Å –≤—ã—Å–æ–∫–æ–π –≤–∞–∂–Ω–æ—Å—Ç—å—é
        new_memory.importance = max(old.importance for old in existing_memories) + 0.1
        db.upsert(new_memory)

# –ü—Ä–∏ retrieval: —Ñ–∏–ª—å—Ç—Ä deprecated
results = db.query(filter={"deprecated": {"$ne": True}})
```

**–°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: LLM-Based Conflict Resolution**
```python
def resolve_conflict_llm(query, conflicting_memories):
    prompt = f"""
    –û–±–Ω–∞—Ä—É–∂–µ–Ω –∫–æ–Ω—Ñ–ª–∏–∫—Ç –≤ –ø–∞–º—è—Ç–∏:

    –ü–∞–º—è—Ç—å 1 ({conflicting_memories[0].timestamp}):
    "{conflicting_memories[0].text}"

    –ü–∞–º—è—Ç—å 2 ({conflicting_memories[1].timestamp}):
    "{conflicting_memories[1].text}"

    –ó–∞–¥–∞—á–∏:
    1. –û–ø—Ä–µ–¥–µ–ª–∏, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∏–ª–∏ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–º –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–µ–º
    2. –ü—Ä–µ–¥–ª–æ–∂–∏ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞
    3. –°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å: "{query}"

    –§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
    –¢–∏–ø –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞: [update/contradiction]
    –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: [–æ–±—ä—è—Å–Ω–µ–Ω–∏–µ]
    –û—Ç–≤–µ—Ç: [—Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç]
    """

    resolution = llm.generate(prompt)

    # Parse –∏ –ø—Ä–∏–º–µ–Ω–∏—Ç—å
    if resolution.type == "update":
        deprecate_old_memories(conflicting_memories[:-1])

    return resolution.answer
```

**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–π –ø–æ–¥—Ö–æ–¥:** –ì–∏–±—Ä–∏–¥ Strategy 2 + 3
- –î–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å explicit updates –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
- –î–ª—è ambiguous cases ‚Üí LLM resolution
- –í—Å–µ–≥–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏—Å—Ç–æ—Ä–∏—é (deprecated=True, –Ω–æ –Ω–µ delete)
</details>

---

**–í–æ–ø—Ä–æ—Å 4.3:** –û–±—ä—è—Å–Ω–∏—Ç–µ –ø—Ä–æ–±–ª–µ–º—É "hallucinated memories" (–ª–æ–∂–Ω—ã—Ö –≤–æ—Å–ø–æ–º–∏–Ω–∞–Ω–∏–π). –ö–∞–∫ –æ–±–µ—Å–ø–µ—á–∏—Ç—å, —á—Ç–æ–±—ã –∞–≥–µ–Ω—Ç –Ω–µ "–ø—Ä–∏–¥—É–º—ã–≤–∞–ª" —Å–æ–±—ã—Ç–∏—è, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ –±—ã–ª–æ?

<details>
<summary>–û—Ç–≤–µ—Ç</summary>

**–ü—Ä–æ–±–ª–µ–º–∞:**
```
User: "–ü–æ–º–Ω–∏—à—å, –º—ã –æ–±—Å—É–∂–¥–∞–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –Ω–∞ –≤—Å—Ç—Ä–µ—á–µ –≤ –æ—Ñ–∏—Å–µ?"
Agent (hallucination): "–î–∞, —Ç—ã —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–ª –ø—Ä–æ CNN –∏ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã!"
‚Üí –í—Å—Ç—Ä–µ—á–∏ –≤ –æ—Ñ–∏—Å–µ –ù–ï –ë–´–õ–û
```

**–ü—Ä–∏—á–∏–Ω—ã:**
1. LLM –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç "–ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–Ω—ã–π" –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ –¥–∞–Ω–Ω—ã—Ö
2. Retrieval –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–æ—Ö–æ–∂–∏–µ (–Ω–æ –Ω–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ) –¥–æ–∫—É–º–µ–Ω—Ç—ã
3. –°–º–µ—à–µ–Ω–∏–µ "–∑–Ω–∞–Ω–∏–π" –º–æ–¥–µ–ª–∏ –∏ –ø–∞–º—è—Ç–∏ –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ

**–†–µ—à–µ–Ω–∏—è:**

**1. Source Attribution (–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏)**
```python
system_prompt = """
CRITICAL: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
–î–ª—è –∫–∞–∂–¥–æ–≥–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è —É–∫–∞–∑—ã–≤–∞–π –∏—Å—Ç–æ—á–Ω–∏–∫ [Doc N].
–ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö, —Å–∫–∞–∂–∏ "–ù–µ –Ω–∞—à—ë–ª –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –≤ –∏—Å—Ç–æ—Ä–∏–∏".

–ù–ï –ò–°–ü–û–õ–¨–ó–£–ô –æ–±—â–∏–µ –∑–Ω–∞–Ω–∏—è –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ.
"""

# –ü—Ä–∏–º–µ—Ä –æ—Ç–≤–µ—Ç–∞:
# "–ú—ã –æ–±—Å—É–∂–¥–∞–ª–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏ –≤ —á–∞—Ç–µ [Doc 15], –Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –æ –≤—Å—Ç—Ä–µ—á–µ –≤ –æ—Ñ–∏—Å–µ –Ω–µ—Ç."
```

**2. Confidence Thresholding**
```python
def answer_with_confidence(query):
    results = vector_db.query(vector=embed(query), top_k=3)

    # Threshold: similarity > 0.75 –¥–ª—è "—É–≤–µ—Ä–µ–Ω–Ω–æ–≥–æ" –æ—Ç–≤–µ—Ç–∞
    if not results or results[0].score < 0.75:
        return "–Ø –Ω–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç—Ç–æ–º –≤ –Ω–∞—à–µ–π –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—â–µ–Ω–∏—è."

    # –ï—Å–ª–∏ –µ—Å—Ç—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å ‚Üí –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å retrieved context
    context = "\n".join([r.text for r in results])

    prompt = f"""
    –ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏:
    {context}

    –í–æ–ø—Ä–æ—Å: {query}

    –í–∞–∂–Ω–æ: –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ.
    –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç ‚Üí —Å–∫–∞–∂–∏ "–ù–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏".
    """

    return llm.generate(prompt)
```

**3. Explicit Memory Markers**
```python
def save_memory(text, metadata):
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è: factual statement vs. question/hypothesis
    is_fact = llm.classify(
        text,
        categories=["factual_statement", "question", "hypothesis", "opinion"]
    )

    if is_fact == "factual_statement":
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –∫–∞–∫ verified memory
        db.upsert({
            "text": text,
            "type": "explicit_fact",
            "verified": True,
            "timestamp": datetime.now(),
            **metadata
        })
    else:
        # –ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∏–ª–∏ –ø–æ–º–µ—Ç–∏—Ç—å –∫–∞–∫ unverified
        db.upsert({
            "text": text,
            "type": "unverified",
            "verified": False,
            **metadata
        })

# –ü—Ä–∏ retrieval: –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç verified facts
results = db.query(
    filter={"verified": True}
)
```

**4. Negative Evidence (–Ø–≤–Ω–æ–µ "–Ω–µ –∑–Ω–∞—é")**
```python
def check_for_event(query):
    # Retrieval
    results = vector_db.query(embed(query), top_k=5)

    # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å—Ç—å –ª–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã?
    relevant = [r for r in results if r.score > 0.7]

    if not relevant:
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å "negative evidence"
        db.upsert({
            "text": f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–ª –ø—Ä–æ '{query}', –Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –Ω–µ—Ç",
            "type": "negative_evidence",
            "query": query,
            "timestamp": datetime.now()
        })

        return "–Ø –Ω–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± —ç—Ç–æ–º –≤ –Ω–∞—à–µ–π –∏—Å—Ç–æ—Ä–∏–∏."

    # –ò–Ω–∞—á–µ: –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ retrieved context
    return generate_answer(query, relevant)

# –ü—Ä–∏ –ø–æ–≤—Ç–æ—Ä–Ω–æ–º –≤–æ–ø—Ä–æ—Å–µ:
# Retrieval –Ω–∞–π–¥—ë—Ç "negative evidence" ‚Üí —Å—Ä–∞–∑—É –æ—Ç–≤–µ—Ç–∏—Ç "–ù–µ –∑–Ω–∞—é"
```

**–õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏:**
1. ‚úÖ –í–°–ï–ì–î–ê —Ç—Ä–µ–±–æ–≤–∞—Ç—å source attribution
2. ‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å confidence thresholds (–Ω–µ –æ—Ç–≤–µ—á–∞—Ç—å –ø—Ä–∏ low similarity)
3. ‚úÖ –†–∞–∑–ª–∏—á–∞—Ç—å verified facts –∏ unverified statements
4. ‚úÖ –°–æ—Ö—Ä–∞–Ω—è—Ç—å "negative evidence" (—á—Ç–æ –ù–ï –æ–±—Å—É–∂–¥–∞–ª–æ—Å—å)
5. ‚úÖ –†–µ–≥—É–ª—è—Ä–Ω—ã–π audit: –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ç–≤–µ—Ç–æ–≤ –Ω–∞ hallucinations
</details>

---

## üéØ –ó–ê–î–ê–ù–ò–Ø

### –ó–∞–¥–∞–Ω–∏–µ 1: –ë–∞–∑–æ–≤–æ–µ (20 –º–∏–Ω—É—Ç)
**–ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã –ø–∞–º—è—Ç–∏ –∞–≥–µ–Ω—Ç–∞**

**–ó–∞–¥–∞—á–∞:**
–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –ø–∞–º—è—Ç–∏ –æ–¥–Ω–æ–≥–æ –∏–∑ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤ (ChatGPT, Character.AI, Notion AI) –∏ —Å–æ–∑–¥–∞–π—Ç–µ diagram —Å –æ–ø–∏—Å–∞–Ω–∏–µ–º –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤.

**–ö—Ä–∏—Ç–µ—Ä–∏–∏:**
1. –ò–¥–µ–Ω—Ç–∏—Ñ–∏—Ü–∏—Ä—É–π—Ç–µ —Ç–∏–ø STM (—Ä–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
2. –û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ –Ω–∞–ª–∏—á–∏–µ LTM (vector DB)
3. –û–ø–∏—à–∏—Ç–µ –º–µ—Ö–∞–Ω–∏–∑–º—ã retrieval
4. –£–∫–∞–∂–∏—Ç–µ —Ç–∏–ø—ã –ø–∞–º—è—Ç–∏ (episodic/semantic/procedural)
5. –û—Ü–µ–Ω–∏—Ç–µ —Å–∏–ª—å–Ω—ã–µ –∏ —Å–ª–∞–±—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã

**–§–æ—Ä–º–∞—Ç:**
```
–ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ü–ê–ú–Ø–¢–ò: [–ù–∞–∑–≤–∞–Ω–∏–µ –∞–≥–µ–Ω—Ç–∞]

1. SHORT-TERM MEMORY
   - –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: X tokens
   - –ú–µ—Ö–∞–Ω–∏–∑–º: [sliding window/summarization/...]
   - –°—Ç–æ–∏–º–æ—Å—Ç—å: $X/1M tokens

2. LONG-TERM MEMORY
   - –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: [Pinecone/Chroma/...]
   - –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è: [HNSW/IVF/...]
   - Retrieval: [semantic/hybrid/...]

3. –¢–ò–ü–´ –ü–ê–ú–Ø–¢–ò
   - Episodic: [–î–∞/–ù–µ—Ç, –ø—Ä–∏–º–µ—Ä—ã]
   - Semantic: [–î–∞/–ù–µ—Ç, –ø—Ä–∏–º–µ—Ä—ã]
   - Procedural: [–î–∞/–ù–µ—Ç, –ø—Ä–∏–º–µ—Ä—ã]

4. –°–ò–õ–¨–ù–´–ï –°–¢–û–†–û–ù–´
   - [–ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç—å]

5. –°–õ–ê–ë–´–ï –°–¢–û–†–û–ù–´
   - [–ü–µ—Ä–µ—á–∏—Å–ª–∏—Ç—å]

6. DIAGRAM
   [ASCII art –∏–ª–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –ø–æ—Ç–æ–∫–∞]
```

**–ü—Ä–∏–º–µ—Ä –Ω–∞—á–∞–ª–∞:**
```
–ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ü–ê–ú–Ø–¢–ò: ChatGPT (GPT-4 + Memory feature)

1. SHORT-TERM MEMORY
   - –†–∞–∑–º–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞: 8K tokens (standard), 128K (extended)
   - –ú–µ—Ö–∞–Ω–∏–∑–º: Sliding window —Å —Å–∂–∞—Ç–∏–µ–º —Å—Ç–∞—Ä—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
   - –°—Ç–æ–∏–º–æ—Å—Ç—å: $0.03/1K prompt + $0.06/1K completion

2. LONG-TERM MEMORY
   - –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö: Proprietary vector DB (–ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–æ Pinecone)
   - –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è: HNSW (–Ω–∞ –æ—Å–Ω–æ–≤–µ performance)
   - Retrieval: Hybrid (semantic + keyword)

[–ø—Ä–æ–¥–æ–ª–∂–∏—Ç—å...]
```

---

### –ó–∞–¥–∞–Ω–∏–µ 2: –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ (35 –º–∏–Ω—É—Ç)
**–î–∏–∑–∞–π–Ω —Å–∏—Å—Ç–µ–º—ã –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ use case**

**–ó–∞–¥–∞—á–∞:**
–°–ø—Ä–æ–µ–∫—Ç–∏—Ä—É–π—Ç–µ —Å–∏—Å—Ç–µ–º—É –ø–∞–º—è—Ç–∏ –¥–ª—è AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ **–∫–æ–¥–∏–Ω–≥–∞** (–∞–Ω–∞–ª–æ–≥ GitHub Copilot Workspace), –∫–æ—Ç–æ—Ä—ã–π –ø–æ–º–æ–≥–∞–µ—Ç —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫—É —Ä–∞–±–æ—Ç–∞—Ç—å —Å –±–æ–ª—å—à–∏–º codebase (10K+ —Ñ–∞–π–ª–æ–≤).

**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**
1. –ê–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø–æ–º–Ω–∏—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ–µ–∫—Ç–∞
2. –ê–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –ø–æ–º–Ω–∏—Ç—å –ø—Ä–æ—à–ª—ã–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∏ –∏ —Ä–µ—à–µ–Ω–∏—è
3. –ê–≥–µ–Ω—Ç –¥–æ–ª–∂–µ–Ω –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è –∫ —Å—Ç–∏–ª—é –∫–æ–¥–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞
4. –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ: –±—é–¥–∂–µ—Ç $100/–º–µ—Å—è—Ü –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è

**–§–æ—Ä–º–∞—Ç:**
```markdown
# –°–ò–°–¢–ï–ú–ê –ü–ê–ú–Ø–¢–ò: AI Coding Assistant

## 1. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê

### Short-Term Memory (STM)
- **–†–∞–∑–º–µ—Ä:** [...]
- **–°–æ–¥–µ—Ä–∂–∏–º–æ–µ:** [—Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª, –æ—Ç–∫—Ä—ã—Ç—ã–µ —Ñ–∞–π–ª—ã, ...]
- **–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ:** [...]

### Long-Term Memory (LTM)
- **–ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö:** [–≤—ã–±–æ—Ä –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ]
- **–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è:** [...]
- **–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:**
  ```json
  {
    "text": "...",
    "embedding": [...],
    "metadata": {
      "file_path": "...",
      "type": "code/comment/commit",
      ...
    }
  }
  ```

### Working Memory
- **–ß—Ç–æ –≤–∫–ª—é—á–∞—Ç—å –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞:** [...]

## 2. –¢–ò–ü–´ –ü–ê–ú–Ø–¢–ò

### Episodic (–≠–ø–∏–∑–æ–¥–∏—á–µ—Å–∫–∞—è)
- **–ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å:** [–ø—Ä–æ—à–ª—ã–µ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∏, bug fixes, ...]
- **–§–æ—Ä–º–∞—Ç:** [...]
- **Retrieval:** [–∫–æ–≥–¥–∞ –∏–∑–≤–ª–µ–∫–∞—Ç—å]

### Semantic (–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è)
- **–ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å:** [—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞, —Å—Ç–∏–ª—å –∫–æ–¥–∞, ...]
- **–§–æ—Ä–º–∞—Ç:** [...]
- **Retrieval:** [...]

### Procedural (–ü—Ä–æ—Ü–µ–¥—É—Ä–Ω–∞—è)
- **–ß—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å:** [–ø–∞—Ç—Ç–µ—Ä–Ω—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è, –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞, ...]
- **–§–æ—Ä–º–∞—Ç:** [...]
- **Retrieval:** [...]

## 3. RETRIEVAL –°–¢–†–ê–¢–ï–ì–ò–Ø

### –ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–æ–¥–∞
```python
def build_context(current_file, task):
    # 1. STM: —Ç–µ–∫—É—â–∏–π —Ñ–∞–π–ª
    context = read_file(current_file)

    # 2. LTM: —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–¥
    similar_code = vector_db.query(...)

    # 3. LTM: –ø—Ä–æ—à–ª—ã–µ —Ä–µ—à–µ–Ω–∏—è
    past_solutions = vector_db.query(...)

    # 4. Procedural: —Å—Ç–∏–ª—å –∫–æ–¥–∞
    style_guide = get_style_preferences(user)

    return merge(context, similar_code, past_solutions, style_guide)
```

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è
- **Caching:** [...]
- **Prefetching:** [...]
- **Compression:** [...]

## 4. BUDGET ANALYSIS

### Vector DB —Å—Ç–æ–∏–º–æ—Å—Ç—å
- **Storage:** 10K —Ñ–∞–π–ª–æ–≤ √ó [—Ä–∞–∑–º–µ—Ä] = [—Å—Ç–æ–∏–º–æ—Å—Ç—å]
- **Queries:** [N –∑–∞–ø—Ä–æ—Å–æ–≤/–¥–µ–Ω—å] √ó [—Ü–µ–Ω–∞] = [—Å—Ç–æ–∏–º–æ—Å—Ç—å]
- **Total:** $X/–º–µ—Å—è—Ü

### LLM —Å—Ç–æ–∏–º–æ—Å—Ç—å
- **Context size:** [average tokens]
- **Requests/day:** [...]
- **Total:** $Y/–º–µ—Å—è—Ü

### –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±—é–¥–∂–µ—Ç–∞
- [—Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Å–Ω–∏–∂–µ–Ω–∏—è —Å—Ç–æ–∏–º–æ—Å—Ç–∏]

## 5. DIAGRAM
[–ù–∞—Ä–∏—Å–æ–≤–∞—Ç—å flow: –∫–æ–¥ ‚Üí chunking ‚Üí embedding ‚Üí storage ‚Üí retrieval ‚Üí generation]
```

**–ë–æ–Ω—É—Å (+10 –º–∏–Ω—É—Ç):** –†–µ–∞–ª–∏–∑—É–π—Ç–µ prototype –Ω–∞ Python —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Chroma DB.

---

### –ó–∞–¥–∞–Ω–∏–µ 3: –ü—Ä–æ–µ–∫—Ç–Ω–æ–µ (60 –º–∏–Ω—É—Ç)
**–°—Ç—Ä–∞—Ç–µ–≥–∏—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é –¥–ª—è –¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–≥–æ –∞–≥–µ–Ω—Ç–∞**

**–ö–æ–Ω—Ç–µ–∫—Å—Ç:**
–í—ã —Ä–∞–∑—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç–µ AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –¥–ª—è **–¥–æ–ª–≥–æ—Å—Ä–æ—á–Ω–æ–π —Ä–∞–±–æ—Ç—ã** (6+ –º–µ—Å—è—Ü–µ–≤) —Å **–æ–¥–Ω–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º**. –ê–≥–µ–Ω—Ç –ø–æ–º–æ–≥–∞–µ—Ç —Å:
- Personal knowledge management (–∑–∞–º–µ—Ç–∫–∏, —Å—Ç–∞—Ç—å–∏, –∫–Ω–∏–≥–∏)
- Task management (TODO, –ø—Ä–æ–µ–∫—Ç—ã)
- Creative writing (—Å—Ü–µ–Ω–∞—Ä–∏–∏, —Ç–µ–∫—Å—Ç—ã)

**–ü—Ä–æ–±–ª–µ–º—ã:**
1. **–ü–µ—Ä–µ–ø–æ–ª–Ω–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏:** 6 –º–µ—Å—è—Ü–µ–≤ √ó 100 —Å–æ–æ–±—â–µ–Ω–∏–π/–¥–µ–Ω—å = 18K —Å–æ–æ–±—â–µ–Ω–∏–π
2. **–ö–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ:** –ò–Ω—Ç–µ—Ä–µ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –º–µ–Ω—è—é—Ç—Å—è
3. **Hallucinations:** –†–∏—Å–∫ "–ø—Ä–∏–¥—É–º—ã–≤–∞–Ω–∏—è" –ø—Ä–æ—à–ª—ã—Ö –æ–±—Å—É–∂–¥–µ–Ω–∏–π
4. **Privacy:** –í—Å—è –∏—Å—Ç–æ—Ä–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ–π (on-device)
5. **Cost:** –ë—é–¥–∂–µ—Ç $0 (self-hosted —Ç–æ–ª—å–∫–æ)

**–ó–∞–¥–∞—á–∞:**
–†–∞–∑—Ä–∞–±–æ—Ç–∞–π—Ç–µ **–ø–æ–ª–Ω—É—é —Å—Ç—Ä–∞—Ç–µ–≥–∏—é —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ø–∞–º—è—Ç—å—é** —Å —É—á—ë—Ç–æ–º –≤—Å–µ—Ö –ø—Ä–æ–±–ª–µ–º.

**–§–æ—Ä–º–∞—Ç:**

```markdown
# –°–¢–†–ê–¢–ï–ì–ò–Ø –£–ü–†–ê–í–õ–ï–ù–ò–Ø –ü–ê–ú–Ø–¢–¨–Æ: Long-Term Personal AI Assistant

## EXECUTIVE SUMMARY
[2-3 –∞–±–∑–∞—Ü–∞: –∫–ª—é—á–µ–≤—ã–µ —Ä–µ—à–µ–Ω–∏—è –∏ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ]

## 1. –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ü–ê–ú–Ø–¢–ò

### 1.1 Multi-Tier Storage
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ HOT TIER (STM)                          ‚îÇ
‚îÇ - Last 10 messages                      ‚îÇ
‚îÇ - Size: 4K tokens                       ‚îÇ
‚îÇ - Storage: In-memory (RAM)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì (overflow)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ WARM TIER (Recent LTM)                  ‚îÇ
‚îÇ - Last 30 days                          ‚îÇ
‚îÇ - Size: ~3K messages                    ‚îÇ
‚îÇ - Storage: Chroma DB (SSD)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚Üì (aging)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ COLD TIER (Archive)                     ‚îÇ
‚îÇ - 30+ days old                          ‚îÇ
‚îÇ - Size: unlimited                       ‚îÇ
‚îÇ - Storage: DuckDB parquet (HDD)         ‚îÇ
‚îÇ - Retrieval: Slower, on-demand          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 1.2 –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
[–î–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞]

## 2. –†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´ –ü–ï–†–ï–ü–û–õ–ù–ï–ù–ò–Ø

### 2.1 Hierarchical Summarization
```python
# –ö–∞–∂–¥—É—é –Ω–µ–¥–µ–ª—é: summary –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 700 —Å–æ–æ–±—â–µ–Ω–∏–π
def weekly_consolidation():
    messages = db.query(last_7_days)

    # LLM summarization
    summary = llm.summarize(
        messages,
        levels=["daily", "weekly"],
        preserve=["important_facts", "decisions", "insights"]
    )

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å summary –∫–∞–∫ high-importance memory
    db.upsert({
        "text": summary,
        "type": "consolidated_summary",
        "importance": 0.9,
        "timespan": "2024-W10",
        "source_count": len(messages)
    })

    # –ü–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª—ã –≤ COLD tier
    archive_to_cold(messages)
```

### 2.2 Importance-Based Retention
[–û–ø–∏—Å–∞–Ω–∏–µ –∞–ª–≥–æ—Ä–∏—Ç–º–∞ importance scoring]

### 2.3 Compression
[–ú–µ—Ç–æ–¥—ã —Å–∂–∞—Ç–∏—è —Å—Ç–∞—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö]

## 3. –†–ï–®–ï–ù–ò–ï –ö–û–ù–§–õ–ò–ö–¢–û–í

### 3.1 Temporal Versioning
```python
class MemoryVersion:
    def __init__(self, text, timestamp, supersedes=None):
        self.text = text
        self.timestamp = timestamp
        self.supersedes = supersedes  # ID —Å—Ç–∞—Ä–æ–π –≤–µ—Ä—Å–∏–∏
        self.deprecated = False

def update_memory(new_info):
    # –ù–∞–π—Ç–∏ –∫–æ–Ω—Ñ–ª–∏–∫—Ç—É—é—â–∏–µ
    conflicts = find_conflicts(new_info)

    for old in conflicts:
        # –ù–µ —É–¥–∞–ª—è—Ç—å, –∞ deprecate
        old.deprecated = True
        old.superseded_by = new_info.id
        db.update(old)

    # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –Ω–æ–≤–æ–µ —Å –ª–∏–Ω–∫–æ–º
    new_info.supersedes = [c.id for c in conflicts]
    db.upsert(new_info)
```

### 3.2 Conflict Detection
[–ê–ª–≥–æ—Ä–∏—Ç–º –¥–µ—Ç–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∫–æ–Ω—Ñ–ª–∏–∫—Ç–æ–≤]

### 3.3 Resolution Strategies
[–ö–∞–∫ —Ä–∞–∑—Ä–µ—à–∞—Ç—å ambiguous cases]

## 4. –ü–†–ï–î–û–¢–í–†–ê–©–ï–ù–ò–ï HALLUCINATIONS

### 4.1 Source Attribution
[–û–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –∏—Å—Ç–æ—á–Ω–∏–∫–∏]

### 4.2 Confidence Scoring
```python
def answer_with_verification(query):
    results = retrieve(query)

    if not results:
        # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å "negative evidence"
        db.upsert({
            "type": "query_without_answer",
            "query": query,
            "timestamp": now()
        })
        return "–ù–µ –Ω–∞—à—ë–ª –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± —ç—Ç–æ–º –≤ –∏—Å—Ç–æ—Ä–∏–∏."

    # Multi-source verification
    if len(results) >= 2 and all(r.score > 0.8 for r in results):
        confidence = "high"
    elif results[0].score > 0.7:
        confidence = "medium"
    else:
        confidence = "low"

    answer = generate(query, results)

    return {
        "answer": answer,
        "confidence": confidence,
        "sources": [r.id for r in results]
    }
```

### 4.3 User Feedback Loop
[–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å feedback –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è]

## 5. PRIVACY & SECURITY

### 5.1 On-Device Storage
```
All data stored locally:
- Vector DB: Chroma (SQLite backend)
- Archive: DuckDB parquet files
- Encryption: AES-256 –¥–ª—è sensitive data
```

### 5.2 Data Isolation
[–ö–∞–∫ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—Ç–∏—Ç—å leakage –º–µ–∂–¥—É —Å–µ—Å—Å–∏—è–º–∏]

### 5.3 Sensitive Data Detection
```python
def classify_sensitivity(text):
    patterns = {
        "password": r"–ø–∞—Ä–æ–ª—å:?\s*\w+",
        "email": r"\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b",
        "credit_card": r"\b\d{4}[\s-]?\d{4}[\s-]?\d{4}[\s-]?\d{4}\b"
    }

    for category, pattern in patterns.items():
        if re.search(pattern, text):
            return category

    return "normal"

# –ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å sensitive data
if classify_sensitivity(text) != "normal":
    return "‚ùå –ù–µ –±—É–¥—É —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ"
```

## 6. COST OPTIMIZATION (Self-Hosted)

### 6.1 Infrastructure
```
Hardware:
- CPU: Apple M1 / AMD Ryzen 5 (–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
- RAM: 16 GB (Chroma DB ~2 GB, LLM 4-8 GB)
- Storage: 256 GB SSD (WARM) + 1 TB HDD (COLD)

Software:
- Vector DB: Chroma (free, open-source)
- LLM: Llama 3 8B (local, free)
- Embedding: sentence-transformers (free)

Total cost: $0/month (–ø–æ—Å–ª–µ initial setup)
```

### 6.2 Optimization Techniques
[Compression, quantization, caching, ...]

## 7. IMPLEMENTATION PLAN

### Phase 1: MVP (Week 1-2)
- [ ] Setup Chroma DB locally
- [ ] Implement basic STM (sliding window)
- [ ] Implement LTM (semantic search)
- [ ] Test with 100 messages

### Phase 2: Advanced Features (Week 3-4)
- [ ] Hierarchical summarization
- [ ] Importance scoring
- [ ] Conflict resolution
- [ ] Hallucination prevention

### Phase 3: Production (Week 5-6)
- [ ] COLD tier (archive)
- [ ] Privacy features (encryption)
- [ ] Performance optimization
- [ ] User testing

## 8. METRICS & EVALUATION

### Success Metrics
- **Recall:** –ê–≥–µ–Ω—Ç –Ω–∞—Ö–æ–¥–∏—Ç 95%+ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏
- **Precision:** <5% false positives (irrelevant retrievals)
- **Latency:** <500ms –¥–ª—è retrieval
- **Storage:** <1 GB –¥–ª—è 6 –º–µ—Å—è—Ü–µ–≤ (18K messages)
- **Hallucination rate:** <2% (user feedback)

### Monitoring
[–ö–∞–∫ –æ—Ç—Å–ª–µ–∂–∏–≤–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –ø–∞–º—è—Ç–∏]

## 9. DIAGRAM
[–ò—Ç–æ–≥–æ–≤–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: flow –æ—Ç user message ‚Üí storage ‚Üí retrieval ‚Üí generation]

## 10. APPENDIX

### A. Code Samples
[–ö–ª—é—á–µ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏]

### B. Alternative Approaches
[–ß—Ç–æ –µ—â–µ —Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–ª–æ—Å—å –∏ –ø–æ—á–µ–º—É –æ—Ç–≤–µ—Ä–≥–Ω—É—Ç–æ]

### C. Future Improvements
[–ß—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å –≤ v2.0]
```

**–ö—Ä–∏—Ç–µ—Ä–∏–∏ –æ—Ü–µ–Ω–∫–∏:**
- ‚úÖ –ü–æ–ª–Ω–æ—Ç–∞ —Ä–µ—à–µ–Ω–∏—è (–≤—Å–µ 5 –ø—Ä–æ–±–ª–µ–º addressed)
- ‚úÖ –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∞—è –≥–ª—É–±–∏–Ω–∞ (concrete implementations, not just ideas)
- ‚úÖ –†–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç—å (–º–æ–∂–Ω–æ —Ä–µ–∞–ª—å–Ω–æ –∏–º–ø–ª–µ–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å)
- ‚úÖ –ò–Ω–Ω–æ–≤–∞—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å (creative solutions)
- ‚úÖ –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (—è—Å–Ω–æ—Å—Ç—å –∏–∑–ª–æ–∂–µ–Ω–∏—è)

---

## üîó –ù–ê–í–ò–ì–ê–¶–ò–Ø

**–ü—Ä–µ–¥—ã–¥—É—â–∞—è –≥–ª–∞–≤–∞:** [05-–ü–õ–ê–ù–ò–†–û–í–ê–ù–ò–ï-–ò-–¶–ï–ü–û–ß–ö–ò-–ú–´–°–õ–ï–ô.md](05-–ü–õ–ê–ù–ò–†–û–í–ê–ù–ò–ï-–ò-–¶–ï–ü–û–ß–ö–ò-–ú–´–°–õ–ï–ô.md)

**–¢–µ–∫—É—â–∞—è –≥–ª–∞–≤–∞:** 07-–ü–ê–ú–Ø–¢–¨-–ê–ì–ï–ù–¢–û–í.md ‚úÖ

**–°–ª–µ–¥—É—é—â–∞—è –≥–ª–∞–≤–∞:** [09-–≠–¢–ò–ö–ê-–ò-–ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨-–ê–ì–ï–ù–¢–û–í.md](09-–≠–¢–ò–ö–ê-–ò-–ë–ï–ó–û–ü–ê–°–ù–û–°–¢–¨-–ê–ì–ï–ù–¢–û–í.md)

**–í–µ—Ä–Ω—É—Ç—å—Å—è –∫ –æ–≥–ª–∞–≤–ª–µ–Ω–∏—é:** [00-–ù–ê–í–ò–ì–ê–¶–ò–Ø-–ü–û-–†–ê–ó–î–ï–õ–£.md](00-–ù–ê–í–ò–ì–ê–¶–ò–Ø-–ü–û-–†–ê–ó–î–ï–õ–£.md)

---

**–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ:** 2025-10-09
**–í–µ—Ä—Å–∏—è:** 2.0 (–¥–æ–±–∞–≤–ª–µ–Ω—ã —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏–µ —Å–µ–∫—Ü–∏–∏, –≤–æ–ø—Ä–æ—Å—ã –∏ –∑–∞–¥–∞–Ω–∏—è)
